{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1cddRTBYG09-Gc0ZG08tyytndDYm95aDK","authorship_tag":"ABX9TyP8ohb+uE6Dv7s7gOzeUZAs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Redes neurais artificiais"],"metadata":{"id":"mPre-YLpAjcj"}},{"cell_type":"markdown","source":["# Base Credit Data"],"metadata":{"id":"axsxtS8XAnBT"}},{"cell_type":"code","source":["# Importando as variáveis\n","\n","import pickle\n","with open('/content/drive/MyDrive/variáveis de teste e treinamento(credit, census)/credit.pkl', 'rb')as f:\n","    x_credit_treinamento, y_credit_treinamento, x_credit_teste, y_credit_teste = pickle.load(f)"],"metadata":{"id":"8lom-sDzAqEE","executionInfo":{"status":"ok","timestamp":1705957447723,"user_tz":180,"elapsed":897,"user":{"displayName":"Kaique F. Santos","userId":"18416165402555736361"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Fazendo o shape das variáveis\n","\n","print(x_credit_treinamento.shape, y_credit_treinamento.shape)\n","print(x_credit_teste.shape, y_credit_teste.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nglvpTqBGAFe","executionInfo":{"status":"ok","timestamp":1705957447723,"user_tz":180,"elapsed":5,"user":{"displayName":"Kaique F. Santos","userId":"18416165402555736361"}},"outputId":"38ffb6f0-6100-4bfb-a3b5-808d7635bfd4"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["(1500, 3) (1500,)\n","(500, 3) (500,)\n"]}]},{"cell_type":"code","source":["# importações necessárias para usar redes neurais\n","\n","from sklearn.neural_network import MLPClassifier # Multilayer Perceptron"],"metadata":{"id":"qomYHlLCGXwm","executionInfo":{"status":"ok","timestamp":1705957448472,"user_tz":180,"elapsed":751,"user":{"displayName":"Kaique F. Santos","userId":"18416165402555736361"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Criando rede neural\n","\n","# rodando por 1000 epochs, mostrando as interações conforme executado\n","# 3 -> 100 -> 100 -> 1\n","# 3 -> 2 -> 2 -> 1\n","rede_neural_credit = MLPClassifier(max_iter=1500, verbose=True, tol=0.0000100, # 0.000100\n","                                   solver='adam', activation='relu', hidden_layer_sizes=(2, 2))\n","rede_neural_credit.fit(x_credit_treinamento, y_credit_treinamento)\n","# 0.00125908\n","\n","# optimization hasn't converged yet. = o algortimo ainda pode diminuir o erro\n","#  Maximum iterations (200) = Rodou por 200 epochs (padrão)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"-vWRTe5hGmnF","executionInfo":{"status":"ok","timestamp":1705957453340,"user_tz":180,"elapsed":4872,"user":{"displayName":"Kaique F. Santos","userId":"18416165402555736361"}},"outputId":"a4a9b404-c8ee-4016-b0dc-597585b16f69"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 1.09796933\n","Iteration 2, loss = 1.08687475\n","Iteration 3, loss = 1.07551285\n","Iteration 4, loss = 1.06378312\n","Iteration 5, loss = 1.05181091\n","Iteration 6, loss = 1.03946664\n","Iteration 7, loss = 1.02665623\n","Iteration 8, loss = 1.01367191\n","Iteration 9, loss = 0.99999421\n","Iteration 10, loss = 0.98624367\n","Iteration 11, loss = 0.97185254\n","Iteration 12, loss = 0.95711985\n","Iteration 13, loss = 0.94199008\n","Iteration 14, loss = 0.92671203\n","Iteration 15, loss = 0.91084985\n","Iteration 16, loss = 0.89486029\n","Iteration 17, loss = 0.87861608\n","Iteration 18, loss = 0.86209986\n","Iteration 19, loss = 0.84582478\n","Iteration 20, loss = 0.82912821\n","Iteration 21, loss = 0.81259765\n","Iteration 22, loss = 0.79584813\n","Iteration 23, loss = 0.77936513\n","Iteration 24, loss = 0.76291835\n","Iteration 25, loss = 0.74661653\n","Iteration 26, loss = 0.73070665\n","Iteration 27, loss = 0.71505785\n","Iteration 28, loss = 0.69977629\n","Iteration 29, loss = 0.68494215\n","Iteration 30, loss = 0.67052603\n","Iteration 31, loss = 0.65650612\n","Iteration 32, loss = 0.64298827\n","Iteration 33, loss = 0.62973589\n","Iteration 34, loss = 0.61716851\n","Iteration 35, loss = 0.60490133\n","Iteration 36, loss = 0.59325254\n","Iteration 37, loss = 0.58209390\n","Iteration 38, loss = 0.57112376\n","Iteration 39, loss = 0.56076454\n","Iteration 40, loss = 0.55087305\n","Iteration 41, loss = 0.54138182\n","Iteration 42, loss = 0.53215793\n","Iteration 43, loss = 0.52338512\n","Iteration 44, loss = 0.51482809\n","Iteration 45, loss = 0.50669306\n","Iteration 46, loss = 0.49885752\n","Iteration 47, loss = 0.49128214\n","Iteration 48, loss = 0.48403048\n","Iteration 49, loss = 0.47694220\n","Iteration 50, loss = 0.47004214\n","Iteration 51, loss = 0.46353739\n","Iteration 52, loss = 0.45696625\n","Iteration 53, loss = 0.45073220\n","Iteration 54, loss = 0.44454734\n","Iteration 55, loss = 0.43851767\n","Iteration 56, loss = 0.43250061\n","Iteration 57, loss = 0.42676088\n","Iteration 58, loss = 0.42101529\n","Iteration 59, loss = 0.41553379\n","Iteration 60, loss = 0.41014155\n","Iteration 61, loss = 0.40486909\n","Iteration 62, loss = 0.39969566\n","Iteration 63, loss = 0.39466993\n","Iteration 64, loss = 0.38946402\n","Iteration 65, loss = 0.38464639\n","Iteration 66, loss = 0.37978605\n","Iteration 67, loss = 0.37504264\n","Iteration 68, loss = 0.37041225\n","Iteration 69, loss = 0.36590794\n","Iteration 70, loss = 0.36131051\n","Iteration 71, loss = 0.35683237\n","Iteration 72, loss = 0.35241833\n","Iteration 73, loss = 0.34806515\n","Iteration 74, loss = 0.34380671\n","Iteration 75, loss = 0.33955290\n","Iteration 76, loss = 0.33544105\n","Iteration 77, loss = 0.33145254\n","Iteration 78, loss = 0.32750726\n","Iteration 79, loss = 0.32357873\n","Iteration 80, loss = 0.31975221\n","Iteration 81, loss = 0.31591749\n","Iteration 82, loss = 0.31233548\n","Iteration 83, loss = 0.30875977\n","Iteration 84, loss = 0.30535396\n","Iteration 85, loss = 0.30188203\n","Iteration 86, loss = 0.29861083\n","Iteration 87, loss = 0.29543138\n","Iteration 88, loss = 0.29245386\n","Iteration 89, loss = 0.28947961\n","Iteration 90, loss = 0.28666465\n","Iteration 91, loss = 0.28395046\n","Iteration 92, loss = 0.28135379\n","Iteration 93, loss = 0.27880165\n","Iteration 94, loss = 0.27643081\n","Iteration 95, loss = 0.27414372\n","Iteration 96, loss = 0.27191977\n","Iteration 97, loss = 0.26976286\n","Iteration 98, loss = 0.26765817\n","Iteration 99, loss = 0.26558187\n","Iteration 100, loss = 0.26365498\n","Iteration 101, loss = 0.26181645\n","Iteration 102, loss = 0.26000891\n","Iteration 103, loss = 0.25832543\n","Iteration 104, loss = 0.25674108\n","Iteration 105, loss = 0.25514598\n","Iteration 106, loss = 0.25371448\n","Iteration 107, loss = 0.25232689\n","Iteration 108, loss = 0.25094216\n","Iteration 109, loss = 0.24968680\n","Iteration 110, loss = 0.24844190\n","Iteration 111, loss = 0.24727125\n","Iteration 112, loss = 0.24610948\n","Iteration 113, loss = 0.24499210\n","Iteration 114, loss = 0.24393372\n","Iteration 115, loss = 0.24291207\n","Iteration 116, loss = 0.24191990\n","Iteration 117, loss = 0.24091108\n","Iteration 118, loss = 0.23993410\n","Iteration 119, loss = 0.23901567\n","Iteration 120, loss = 0.23807235\n","Iteration 121, loss = 0.23715401\n","Iteration 122, loss = 0.23624748\n","Iteration 123, loss = 0.23536476\n","Iteration 124, loss = 0.23445143\n","Iteration 125, loss = 0.23357305\n","Iteration 126, loss = 0.23267160\n","Iteration 127, loss = 0.23180145\n","Iteration 128, loss = 0.23092744\n","Iteration 129, loss = 0.23005955\n","Iteration 130, loss = 0.22919992\n","Iteration 131, loss = 0.22835557\n","Iteration 132, loss = 0.22749689\n","Iteration 133, loss = 0.22667718\n","Iteration 134, loss = 0.22582017\n","Iteration 135, loss = 0.22500635\n","Iteration 136, loss = 0.22418345\n","Iteration 137, loss = 0.22337276\n","Iteration 138, loss = 0.22256753\n","Iteration 139, loss = 0.22177418\n","Iteration 140, loss = 0.22098771\n","Iteration 141, loss = 0.22022370\n","Iteration 142, loss = 0.21947992\n","Iteration 143, loss = 0.21873139\n","Iteration 144, loss = 0.21800364\n","Iteration 145, loss = 0.21728988\n","Iteration 146, loss = 0.21657548\n","Iteration 147, loss = 0.21586416\n","Iteration 148, loss = 0.21516720\n","Iteration 149, loss = 0.21448090\n","Iteration 150, loss = 0.21378560\n","Iteration 151, loss = 0.21310263\n","Iteration 152, loss = 0.21241897\n","Iteration 153, loss = 0.21172338\n","Iteration 154, loss = 0.21104707\n","Iteration 155, loss = 0.21035882\n","Iteration 156, loss = 0.20969834\n","Iteration 157, loss = 0.20901786\n","Iteration 158, loss = 0.20833440\n","Iteration 159, loss = 0.20765413\n","Iteration 160, loss = 0.20701226\n","Iteration 161, loss = 0.20631704\n","Iteration 162, loss = 0.20565089\n","Iteration 163, loss = 0.20499096\n","Iteration 164, loss = 0.20432135\n","Iteration 165, loss = 0.20365499\n","Iteration 166, loss = 0.20296952\n","Iteration 167, loss = 0.20230942\n","Iteration 168, loss = 0.20164987\n","Iteration 169, loss = 0.20100179\n","Iteration 170, loss = 0.20035523\n","Iteration 171, loss = 0.19970495\n","Iteration 172, loss = 0.19906505\n","Iteration 173, loss = 0.19841897\n","Iteration 174, loss = 0.19777695\n","Iteration 175, loss = 0.19712760\n","Iteration 176, loss = 0.19647553\n","Iteration 177, loss = 0.19581817\n","Iteration 178, loss = 0.19514465\n","Iteration 179, loss = 0.19446380\n","Iteration 180, loss = 0.19376096\n","Iteration 181, loss = 0.19306666\n","Iteration 182, loss = 0.19235726\n","Iteration 183, loss = 0.19164122\n","Iteration 184, loss = 0.19091131\n","Iteration 185, loss = 0.19018693\n","Iteration 186, loss = 0.18945666\n","Iteration 187, loss = 0.18871084\n","Iteration 188, loss = 0.18798330\n","Iteration 189, loss = 0.18724995\n","Iteration 190, loss = 0.18652700\n","Iteration 191, loss = 0.18579063\n","Iteration 192, loss = 0.18502757\n","Iteration 193, loss = 0.18429787\n","Iteration 194, loss = 0.18351822\n","Iteration 195, loss = 0.18276904\n","Iteration 196, loss = 0.18203283\n","Iteration 197, loss = 0.18128991\n","Iteration 198, loss = 0.18055033\n","Iteration 199, loss = 0.17982781\n","Iteration 200, loss = 0.17909621\n","Iteration 201, loss = 0.17836960\n","Iteration 202, loss = 0.17765929\n","Iteration 203, loss = 0.17693680\n","Iteration 204, loss = 0.17622732\n","Iteration 205, loss = 0.17546988\n","Iteration 206, loss = 0.17474285\n","Iteration 207, loss = 0.17398446\n","Iteration 208, loss = 0.17321709\n","Iteration 209, loss = 0.17244321\n","Iteration 210, loss = 0.17172170\n","Iteration 211, loss = 0.17097421\n","Iteration 212, loss = 0.17022310\n","Iteration 213, loss = 0.16952244\n","Iteration 214, loss = 0.16873292\n","Iteration 215, loss = 0.16796982\n","Iteration 216, loss = 0.16721956\n","Iteration 217, loss = 0.16646036\n","Iteration 218, loss = 0.16569280\n","Iteration 219, loss = 0.16498327\n","Iteration 220, loss = 0.16423494\n","Iteration 221, loss = 0.16351644\n","Iteration 222, loss = 0.16280668\n","Iteration 223, loss = 0.16208994\n","Iteration 224, loss = 0.16137574\n","Iteration 225, loss = 0.16065863\n","Iteration 226, loss = 0.15993499\n","Iteration 227, loss = 0.15925471\n","Iteration 228, loss = 0.15853915\n","Iteration 229, loss = 0.15784521\n","Iteration 230, loss = 0.15714028\n","Iteration 231, loss = 0.15646657\n","Iteration 232, loss = 0.15578573\n","Iteration 233, loss = 0.15513066\n","Iteration 234, loss = 0.15450261\n","Iteration 235, loss = 0.15385265\n","Iteration 236, loss = 0.15322063\n","Iteration 237, loss = 0.15255808\n","Iteration 238, loss = 0.15192418\n","Iteration 239, loss = 0.15129465\n","Iteration 240, loss = 0.15068393\n","Iteration 241, loss = 0.15004747\n","Iteration 242, loss = 0.14941138\n","Iteration 243, loss = 0.14879162\n","Iteration 244, loss = 0.14817077\n","Iteration 245, loss = 0.14755319\n","Iteration 246, loss = 0.14695987\n","Iteration 247, loss = 0.14638101\n","Iteration 248, loss = 0.14576924\n","Iteration 249, loss = 0.14516036\n","Iteration 250, loss = 0.14457495\n","Iteration 251, loss = 0.14401704\n","Iteration 252, loss = 0.14342090\n","Iteration 253, loss = 0.14285196\n","Iteration 254, loss = 0.14227939\n","Iteration 255, loss = 0.14168757\n","Iteration 256, loss = 0.14112481\n","Iteration 257, loss = 0.14055344\n","Iteration 258, loss = 0.13999854\n","Iteration 259, loss = 0.13942559\n","Iteration 260, loss = 0.13885914\n","Iteration 261, loss = 0.13830166\n","Iteration 262, loss = 0.13776230\n","Iteration 263, loss = 0.13716786\n","Iteration 264, loss = 0.13662571\n","Iteration 265, loss = 0.13604936\n","Iteration 266, loss = 0.13549161\n","Iteration 267, loss = 0.13493649\n","Iteration 268, loss = 0.13438295\n","Iteration 269, loss = 0.13381712\n","Iteration 270, loss = 0.13329019\n","Iteration 271, loss = 0.13273396\n","Iteration 272, loss = 0.13217091\n","Iteration 273, loss = 0.13163366\n","Iteration 274, loss = 0.13109857\n","Iteration 275, loss = 0.13058261\n","Iteration 276, loss = 0.13001419\n","Iteration 277, loss = 0.12947515\n","Iteration 278, loss = 0.12894564\n","Iteration 279, loss = 0.12840805\n","Iteration 280, loss = 0.12786762\n","Iteration 281, loss = 0.12733826\n","Iteration 282, loss = 0.12681937\n","Iteration 283, loss = 0.12628660\n","Iteration 284, loss = 0.12575624\n","Iteration 285, loss = 0.12524476\n","Iteration 286, loss = 0.12471796\n","Iteration 287, loss = 0.12418591\n","Iteration 288, loss = 0.12367090\n","Iteration 289, loss = 0.12314658\n","Iteration 290, loss = 0.12264178\n","Iteration 291, loss = 0.12211805\n","Iteration 292, loss = 0.12158615\n","Iteration 293, loss = 0.12106104\n","Iteration 294, loss = 0.12053758\n","Iteration 295, loss = 0.12005146\n","Iteration 296, loss = 0.11952371\n","Iteration 297, loss = 0.11900232\n","Iteration 298, loss = 0.11850617\n","Iteration 299, loss = 0.11798706\n","Iteration 300, loss = 0.11752008\n","Iteration 301, loss = 0.11697058\n","Iteration 302, loss = 0.11646348\n","Iteration 303, loss = 0.11595498\n","Iteration 304, loss = 0.11546308\n","Iteration 305, loss = 0.11496699\n","Iteration 306, loss = 0.11445545\n","Iteration 307, loss = 0.11397518\n","Iteration 308, loss = 0.11348695\n","Iteration 309, loss = 0.11299702\n","Iteration 310, loss = 0.11248493\n","Iteration 311, loss = 0.11201400\n","Iteration 312, loss = 0.11149763\n","Iteration 313, loss = 0.11101340\n","Iteration 314, loss = 0.11052019\n","Iteration 315, loss = 0.11003510\n","Iteration 316, loss = 0.10955096\n","Iteration 317, loss = 0.10908567\n","Iteration 318, loss = 0.10858317\n","Iteration 319, loss = 0.10811939\n","Iteration 320, loss = 0.10763178\n","Iteration 321, loss = 0.10715802\n","Iteration 322, loss = 0.10667727\n","Iteration 323, loss = 0.10620430\n","Iteration 324, loss = 0.10572037\n","Iteration 325, loss = 0.10524678\n","Iteration 326, loss = 0.10476724\n","Iteration 327, loss = 0.10430605\n","Iteration 328, loss = 0.10383324\n","Iteration 329, loss = 0.10335244\n","Iteration 330, loss = 0.10290188\n","Iteration 331, loss = 0.10243412\n","Iteration 332, loss = 0.10198075\n","Iteration 333, loss = 0.10150747\n","Iteration 334, loss = 0.10104305\n","Iteration 335, loss = 0.10058942\n","Iteration 336, loss = 0.10012407\n","Iteration 337, loss = 0.09965417\n","Iteration 338, loss = 0.09920365\n","Iteration 339, loss = 0.09874888\n","Iteration 340, loss = 0.09830557\n","Iteration 341, loss = 0.09783577\n","Iteration 342, loss = 0.09740641\n","Iteration 343, loss = 0.09694535\n","Iteration 344, loss = 0.09650697\n","Iteration 345, loss = 0.09606127\n","Iteration 346, loss = 0.09560519\n","Iteration 347, loss = 0.09519498\n","Iteration 348, loss = 0.09471960\n","Iteration 349, loss = 0.09429154\n","Iteration 350, loss = 0.09383751\n","Iteration 351, loss = 0.09341982\n","Iteration 352, loss = 0.09298232\n","Iteration 353, loss = 0.09253522\n","Iteration 354, loss = 0.09210284\n","Iteration 355, loss = 0.09168318\n","Iteration 356, loss = 0.09125737\n","Iteration 357, loss = 0.09082648\n","Iteration 358, loss = 0.09038817\n","Iteration 359, loss = 0.08996097\n","Iteration 360, loss = 0.08953441\n","Iteration 361, loss = 0.08911401\n","Iteration 362, loss = 0.08870305\n","Iteration 363, loss = 0.08827906\n","Iteration 364, loss = 0.08784085\n","Iteration 365, loss = 0.08745485\n","Iteration 366, loss = 0.08701640\n","Iteration 367, loss = 0.08660234\n","Iteration 368, loss = 0.08618999\n","Iteration 369, loss = 0.08577847\n","Iteration 370, loss = 0.08537235\n","Iteration 371, loss = 0.08495029\n","Iteration 372, loss = 0.08456502\n","Iteration 373, loss = 0.08414394\n","Iteration 374, loss = 0.08373177\n","Iteration 375, loss = 0.08334153\n","Iteration 376, loss = 0.08295568\n","Iteration 377, loss = 0.08254233\n","Iteration 378, loss = 0.08214938\n","Iteration 379, loss = 0.08180473\n","Iteration 380, loss = 0.08155284\n","Iteration 381, loss = 0.08127159\n","Iteration 382, loss = 0.08099807\n","Iteration 383, loss = 0.08073263\n","Iteration 384, loss = 0.08047872\n","Iteration 385, loss = 0.08021019\n","Iteration 386, loss = 0.07995391\n","Iteration 387, loss = 0.07970410\n","Iteration 388, loss = 0.07946304\n","Iteration 389, loss = 0.07920781\n","Iteration 390, loss = 0.07896174\n","Iteration 391, loss = 0.07872202\n","Iteration 392, loss = 0.07846556\n","Iteration 393, loss = 0.07821352\n","Iteration 394, loss = 0.07798141\n","Iteration 395, loss = 0.07774199\n","Iteration 396, loss = 0.07748525\n","Iteration 397, loss = 0.07726361\n","Iteration 398, loss = 0.07701751\n","Iteration 399, loss = 0.07678003\n","Iteration 400, loss = 0.07655523\n","Iteration 401, loss = 0.07632152\n","Iteration 402, loss = 0.07607894\n","Iteration 403, loss = 0.07584124\n","Iteration 404, loss = 0.07562997\n","Iteration 405, loss = 0.07538335\n","Iteration 406, loss = 0.07516078\n","Iteration 407, loss = 0.07494584\n","Iteration 408, loss = 0.07469911\n","Iteration 409, loss = 0.07448222\n","Iteration 410, loss = 0.07425425\n","Iteration 411, loss = 0.07404068\n","Iteration 412, loss = 0.07380365\n","Iteration 413, loss = 0.07361058\n","Iteration 414, loss = 0.07337489\n","Iteration 415, loss = 0.07316474\n","Iteration 416, loss = 0.07294796\n","Iteration 417, loss = 0.07273964\n","Iteration 418, loss = 0.07252328\n","Iteration 419, loss = 0.07230628\n","Iteration 420, loss = 0.07210580\n","Iteration 421, loss = 0.07189419\n","Iteration 422, loss = 0.07166950\n","Iteration 423, loss = 0.07145446\n","Iteration 424, loss = 0.07124508\n","Iteration 425, loss = 0.07105047\n","Iteration 426, loss = 0.07083673\n","Iteration 427, loss = 0.07062119\n","Iteration 428, loss = 0.07041223\n","Iteration 429, loss = 0.07021169\n","Iteration 430, loss = 0.07001870\n","Iteration 431, loss = 0.06981319\n","Iteration 432, loss = 0.06959416\n","Iteration 433, loss = 0.06940785\n","Iteration 434, loss = 0.06920205\n","Iteration 435, loss = 0.06900964\n","Iteration 436, loss = 0.06880121\n","Iteration 437, loss = 0.06860897\n","Iteration 438, loss = 0.06840527\n","Iteration 439, loss = 0.06821661\n","Iteration 440, loss = 0.06802309\n","Iteration 441, loss = 0.06782138\n","Iteration 442, loss = 0.06762620\n","Iteration 443, loss = 0.06744270\n","Iteration 444, loss = 0.06724942\n","Iteration 445, loss = 0.06706284\n","Iteration 446, loss = 0.06687370\n","Iteration 447, loss = 0.06668323\n","Iteration 448, loss = 0.06649086\n","Iteration 449, loss = 0.06630727\n","Iteration 450, loss = 0.06610792\n","Iteration 451, loss = 0.06592845\n","Iteration 452, loss = 0.06574232\n","Iteration 453, loss = 0.06554452\n","Iteration 454, loss = 0.06535315\n","Iteration 455, loss = 0.06517015\n","Iteration 456, loss = 0.06497841\n","Iteration 457, loss = 0.06480999\n","Iteration 458, loss = 0.06460756\n","Iteration 459, loss = 0.06442477\n","Iteration 460, loss = 0.06426356\n","Iteration 461, loss = 0.06406126\n","Iteration 462, loss = 0.06392845\n","Iteration 463, loss = 0.06369100\n","Iteration 464, loss = 0.06350576\n","Iteration 465, loss = 0.06335385\n","Iteration 466, loss = 0.06317096\n","Iteration 467, loss = 0.06297768\n","Iteration 468, loss = 0.06280418\n","Iteration 469, loss = 0.06263617\n","Iteration 470, loss = 0.06246130\n","Iteration 471, loss = 0.06226174\n","Iteration 472, loss = 0.06209656\n","Iteration 473, loss = 0.06191598\n","Iteration 474, loss = 0.06175049\n","Iteration 475, loss = 0.06155085\n","Iteration 476, loss = 0.06137402\n","Iteration 477, loss = 0.06119836\n","Iteration 478, loss = 0.06102349\n","Iteration 479, loss = 0.06086666\n","Iteration 480, loss = 0.06068630\n","Iteration 481, loss = 0.06050568\n","Iteration 482, loss = 0.06034141\n","Iteration 483, loss = 0.06015990\n","Iteration 484, loss = 0.05999804\n","Iteration 485, loss = 0.05982209\n","Iteration 486, loss = 0.05964040\n","Iteration 487, loss = 0.05947774\n","Iteration 488, loss = 0.05930263\n","Iteration 489, loss = 0.05913884\n","Iteration 490, loss = 0.05897255\n","Iteration 491, loss = 0.05880100\n","Iteration 492, loss = 0.05862746\n","Iteration 493, loss = 0.05846441\n","Iteration 494, loss = 0.05829938\n","Iteration 495, loss = 0.05812500\n","Iteration 496, loss = 0.05796168\n","Iteration 497, loss = 0.05779941\n","Iteration 498, loss = 0.05764509\n","Iteration 499, loss = 0.05746734\n","Iteration 500, loss = 0.05730498\n","Iteration 501, loss = 0.05714236\n","Iteration 502, loss = 0.05698677\n","Iteration 503, loss = 0.05682322\n","Iteration 504, loss = 0.05666360\n","Iteration 505, loss = 0.05650022\n","Iteration 506, loss = 0.05634302\n","Iteration 507, loss = 0.05618449\n","Iteration 508, loss = 0.05601295\n","Iteration 509, loss = 0.05586181\n","Iteration 510, loss = 0.05571215\n","Iteration 511, loss = 0.05556520\n","Iteration 512, loss = 0.05539450\n","Iteration 513, loss = 0.05523582\n","Iteration 514, loss = 0.05508617\n","Iteration 515, loss = 0.05491688\n","Iteration 516, loss = 0.05476295\n","Iteration 517, loss = 0.05459794\n","Iteration 518, loss = 0.05445385\n","Iteration 519, loss = 0.05430000\n","Iteration 520, loss = 0.05414770\n","Iteration 521, loss = 0.05398067\n","Iteration 522, loss = 0.05383599\n","Iteration 523, loss = 0.05367972\n","Iteration 524, loss = 0.05352163\n","Iteration 525, loss = 0.05337418\n","Iteration 526, loss = 0.05322863\n","Iteration 527, loss = 0.05310500\n","Iteration 528, loss = 0.05293276\n","Iteration 529, loss = 0.05278261\n","Iteration 530, loss = 0.05265048\n","Iteration 531, loss = 0.05247676\n","Iteration 532, loss = 0.05233158\n","Iteration 533, loss = 0.05219715\n","Iteration 534, loss = 0.05202368\n","Iteration 535, loss = 0.05187869\n","Iteration 536, loss = 0.05172956\n","Iteration 537, loss = 0.05158137\n","Iteration 538, loss = 0.05145760\n","Iteration 539, loss = 0.05130543\n","Iteration 540, loss = 0.05114731\n","Iteration 541, loss = 0.05102332\n","Iteration 542, loss = 0.05086098\n","Iteration 543, loss = 0.05072186\n","Iteration 544, loss = 0.05057937\n","Iteration 545, loss = 0.05043952\n","Iteration 546, loss = 0.05028981\n","Iteration 547, loss = 0.05015302\n","Iteration 548, loss = 0.05001430\n","Iteration 549, loss = 0.04986153\n","Iteration 550, loss = 0.04972199\n","Iteration 551, loss = 0.04959378\n","Iteration 552, loss = 0.04944210\n","Iteration 553, loss = 0.04930935\n","Iteration 554, loss = 0.04917211\n","Iteration 555, loss = 0.04903217\n","Iteration 556, loss = 0.04889284\n","Iteration 557, loss = 0.04873936\n","Iteration 558, loss = 0.04860884\n","Iteration 559, loss = 0.04847418\n","Iteration 560, loss = 0.04833499\n","Iteration 561, loss = 0.04819346\n","Iteration 562, loss = 0.04806245\n","Iteration 563, loss = 0.04793825\n","Iteration 564, loss = 0.04779301\n","Iteration 565, loss = 0.04766009\n","Iteration 566, loss = 0.04749915\n","Iteration 567, loss = 0.04737174\n","Iteration 568, loss = 0.04726665\n","Iteration 569, loss = 0.04710257\n","Iteration 570, loss = 0.04696422\n","Iteration 571, loss = 0.04684865\n","Iteration 572, loss = 0.04670091\n","Iteration 573, loss = 0.04656776\n","Iteration 574, loss = 0.04644468\n","Iteration 575, loss = 0.04630773\n","Iteration 576, loss = 0.04618636\n","Iteration 577, loss = 0.04603931\n","Iteration 578, loss = 0.04590643\n","Iteration 579, loss = 0.04578051\n","Iteration 580, loss = 0.04564845\n","Iteration 581, loss = 0.04551662\n","Iteration 582, loss = 0.04538377\n","Iteration 583, loss = 0.04526279\n","Iteration 584, loss = 0.04513765\n","Iteration 585, loss = 0.04500730\n","Iteration 586, loss = 0.04488209\n","Iteration 587, loss = 0.04475156\n","Iteration 588, loss = 0.04462456\n","Iteration 589, loss = 0.04449387\n","Iteration 590, loss = 0.04437710\n","Iteration 591, loss = 0.04424231\n","Iteration 592, loss = 0.04411217\n","Iteration 593, loss = 0.04398930\n","Iteration 594, loss = 0.04385372\n","Iteration 595, loss = 0.04373678\n","Iteration 596, loss = 0.04362210\n","Iteration 597, loss = 0.04347948\n","Iteration 598, loss = 0.04335718\n","Iteration 599, loss = 0.04324478\n","Iteration 600, loss = 0.04312034\n","Iteration 601, loss = 0.04299506\n","Iteration 602, loss = 0.04286008\n","Iteration 603, loss = 0.04273826\n","Iteration 604, loss = 0.04262133\n","Iteration 605, loss = 0.04250578\n","Iteration 606, loss = 0.04238247\n","Iteration 607, loss = 0.04227241\n","Iteration 608, loss = 0.04214419\n","Iteration 609, loss = 0.04200721\n","Iteration 610, loss = 0.04189709\n","Iteration 611, loss = 0.04178812\n","Iteration 612, loss = 0.04164218\n","Iteration 613, loss = 0.04154732\n","Iteration 614, loss = 0.04140290\n","Iteration 615, loss = 0.04131894\n","Iteration 616, loss = 0.04118322\n","Iteration 617, loss = 0.04105290\n","Iteration 618, loss = 0.04093714\n","Iteration 619, loss = 0.04081920\n","Iteration 620, loss = 0.04072580\n","Iteration 621, loss = 0.04059123\n","Iteration 622, loss = 0.04049534\n","Iteration 623, loss = 0.04036755\n","Iteration 624, loss = 0.04024970\n","Iteration 625, loss = 0.04014120\n","Iteration 626, loss = 0.04000395\n","Iteration 627, loss = 0.03989427\n","Iteration 628, loss = 0.03980273\n","Iteration 629, loss = 0.03967527\n","Iteration 630, loss = 0.03955789\n","Iteration 631, loss = 0.03945834\n","Iteration 632, loss = 0.03935520\n","Iteration 633, loss = 0.03923185\n","Iteration 634, loss = 0.03911218\n","Iteration 635, loss = 0.03900676\n","Iteration 636, loss = 0.03888573\n","Iteration 637, loss = 0.03880559\n","Iteration 638, loss = 0.03866941\n","Iteration 639, loss = 0.03856587\n","Iteration 640, loss = 0.03845642\n","Iteration 641, loss = 0.03834400\n","Iteration 642, loss = 0.03822926\n","Iteration 643, loss = 0.03812841\n","Iteration 644, loss = 0.03801429\n","Iteration 645, loss = 0.03792579\n","Iteration 646, loss = 0.03779710\n","Iteration 647, loss = 0.03768973\n","Iteration 648, loss = 0.03758522\n","Iteration 649, loss = 0.03748464\n","Iteration 650, loss = 0.03738101\n","Iteration 651, loss = 0.03727594\n","Iteration 652, loss = 0.03716214\n","Iteration 653, loss = 0.03705248\n","Iteration 654, loss = 0.03694520\n","Iteration 655, loss = 0.03684565\n","Iteration 656, loss = 0.03673739\n","Iteration 657, loss = 0.03662623\n","Iteration 658, loss = 0.03653178\n","Iteration 659, loss = 0.03642731\n","Iteration 660, loss = 0.03633217\n","Iteration 661, loss = 0.03623075\n","Iteration 662, loss = 0.03612057\n","Iteration 663, loss = 0.03601564\n","Iteration 664, loss = 0.03590259\n","Iteration 665, loss = 0.03579944\n","Iteration 666, loss = 0.03570025\n","Iteration 667, loss = 0.03561998\n","Iteration 668, loss = 0.03549628\n","Iteration 669, loss = 0.03538379\n","Iteration 670, loss = 0.03529475\n","Iteration 671, loss = 0.03519134\n","Iteration 672, loss = 0.03508637\n","Iteration 673, loss = 0.03498216\n","Iteration 674, loss = 0.03488964\n","Iteration 675, loss = 0.03480673\n","Iteration 676, loss = 0.03469478\n","Iteration 677, loss = 0.03458149\n","Iteration 678, loss = 0.03447903\n","Iteration 679, loss = 0.03440987\n","Iteration 680, loss = 0.03427775\n","Iteration 681, loss = 0.03420505\n","Iteration 682, loss = 0.03412366\n","Iteration 683, loss = 0.03399906\n","Iteration 684, loss = 0.03388823\n","Iteration 685, loss = 0.03381580\n","Iteration 686, loss = 0.03370380\n","Iteration 687, loss = 0.03359914\n","Iteration 688, loss = 0.03350008\n","Iteration 689, loss = 0.03342215\n","Iteration 690, loss = 0.03333218\n","Iteration 691, loss = 0.03322088\n","Iteration 692, loss = 0.03313237\n","Iteration 693, loss = 0.03303508\n","Iteration 694, loss = 0.03293969\n","Iteration 695, loss = 0.03283813\n","Iteration 696, loss = 0.03276615\n","Iteration 697, loss = 0.03264377\n","Iteration 698, loss = 0.03255871\n","Iteration 699, loss = 0.03249756\n","Iteration 700, loss = 0.03237791\n","Iteration 701, loss = 0.03229592\n","Iteration 702, loss = 0.03218822\n","Iteration 703, loss = 0.03208906\n","Iteration 704, loss = 0.03201460\n","Iteration 705, loss = 0.03191219\n","Iteration 706, loss = 0.03183361\n","Iteration 707, loss = 0.03173214\n","Iteration 708, loss = 0.03165909\n","Iteration 709, loss = 0.03155695\n","Iteration 710, loss = 0.03146548\n","Iteration 711, loss = 0.03137781\n","Iteration 712, loss = 0.03127329\n","Iteration 713, loss = 0.03118397\n","Iteration 714, loss = 0.03110709\n","Iteration 715, loss = 0.03101082\n","Iteration 716, loss = 0.03093930\n","Iteration 717, loss = 0.03084804\n","Iteration 718, loss = 0.03074481\n","Iteration 719, loss = 0.03067907\n","Iteration 720, loss = 0.03058629\n","Iteration 721, loss = 0.03047256\n","Iteration 722, loss = 0.03040668\n","Iteration 723, loss = 0.03028750\n","Iteration 724, loss = 0.03021418\n","Iteration 725, loss = 0.03012755\n","Iteration 726, loss = 0.03004576\n","Iteration 727, loss = 0.02995195\n","Iteration 728, loss = 0.02986158\n","Iteration 729, loss = 0.02977850\n","Iteration 730, loss = 0.02971227\n","Iteration 731, loss = 0.02964095\n","Iteration 732, loss = 0.02952793\n","Iteration 733, loss = 0.02944982\n","Iteration 734, loss = 0.02937038\n","Iteration 735, loss = 0.02929639\n","Iteration 736, loss = 0.02920574\n","Iteration 737, loss = 0.02910982\n","Iteration 738, loss = 0.02902779\n","Iteration 739, loss = 0.02894184\n","Iteration 740, loss = 0.02885244\n","Iteration 741, loss = 0.02877998\n","Iteration 742, loss = 0.02869106\n","Iteration 743, loss = 0.02861503\n","Iteration 744, loss = 0.02853326\n","Iteration 745, loss = 0.02844811\n","Iteration 746, loss = 0.02836484\n","Iteration 747, loss = 0.02827714\n","Iteration 748, loss = 0.02820215\n","Iteration 749, loss = 0.02812658\n","Iteration 750, loss = 0.02805060\n","Iteration 751, loss = 0.02796532\n","Iteration 752, loss = 0.02787826\n","Iteration 753, loss = 0.02780012\n","Iteration 754, loss = 0.02772475\n","Iteration 755, loss = 0.02763791\n","Iteration 756, loss = 0.02755970\n","Iteration 757, loss = 0.02748897\n","Iteration 758, loss = 0.02740985\n","Iteration 759, loss = 0.02733031\n","Iteration 760, loss = 0.02726497\n","Iteration 761, loss = 0.02718687\n","Iteration 762, loss = 0.02710771\n","Iteration 763, loss = 0.02701229\n","Iteration 764, loss = 0.02694850\n","Iteration 765, loss = 0.02685213\n","Iteration 766, loss = 0.02678436\n","Iteration 767, loss = 0.02670403\n","Iteration 768, loss = 0.02662251\n","Iteration 769, loss = 0.02655350\n","Iteration 770, loss = 0.02647895\n","Iteration 771, loss = 0.02640792\n","Iteration 772, loss = 0.02633768\n","Iteration 773, loss = 0.02626648\n","Iteration 774, loss = 0.02618408\n","Iteration 775, loss = 0.02609724\n","Iteration 776, loss = 0.02601348\n","Iteration 777, loss = 0.02594547\n","Iteration 778, loss = 0.02587541\n","Iteration 779, loss = 0.02579633\n","Iteration 780, loss = 0.02571608\n","Iteration 781, loss = 0.02565336\n","Iteration 782, loss = 0.02557598\n","Iteration 783, loss = 0.02549581\n","Iteration 784, loss = 0.02542848\n","Iteration 785, loss = 0.02534874\n","Iteration 786, loss = 0.02528488\n","Iteration 787, loss = 0.02522152\n","Iteration 788, loss = 0.02513064\n","Iteration 789, loss = 0.02508106\n","Iteration 790, loss = 0.02500226\n","Iteration 791, loss = 0.02492705\n","Iteration 792, loss = 0.02484263\n","Iteration 793, loss = 0.02478938\n","Iteration 794, loss = 0.02470234\n","Iteration 795, loss = 0.02464178\n","Iteration 796, loss = 0.02456378\n","Iteration 797, loss = 0.02449033\n","Iteration 798, loss = 0.02444440\n","Iteration 799, loss = 0.02435872\n","Iteration 800, loss = 0.02427942\n","Iteration 801, loss = 0.02421277\n","Iteration 802, loss = 0.02415264\n","Iteration 803, loss = 0.02406918\n","Iteration 804, loss = 0.02401152\n","Iteration 805, loss = 0.02393383\n","Iteration 806, loss = 0.02390244\n","Iteration 807, loss = 0.02379697\n","Iteration 808, loss = 0.02374907\n","Iteration 809, loss = 0.02366751\n","Iteration 810, loss = 0.02359871\n","Iteration 811, loss = 0.02352802\n","Iteration 812, loss = 0.02344941\n","Iteration 813, loss = 0.02339080\n","Iteration 814, loss = 0.02331403\n","Iteration 815, loss = 0.02326069\n","Iteration 816, loss = 0.02320133\n","Iteration 817, loss = 0.02312001\n","Iteration 818, loss = 0.02306697\n","Iteration 819, loss = 0.02299477\n","Iteration 820, loss = 0.02293310\n","Iteration 821, loss = 0.02288570\n","Iteration 822, loss = 0.02280571\n","Iteration 823, loss = 0.02273448\n","Iteration 824, loss = 0.02265225\n","Iteration 825, loss = 0.02259449\n","Iteration 826, loss = 0.02252263\n","Iteration 827, loss = 0.02246088\n","Iteration 828, loss = 0.02240944\n","Iteration 829, loss = 0.02232572\n","Iteration 830, loss = 0.02230219\n","Iteration 831, loss = 0.02221380\n","Iteration 832, loss = 0.02217135\n","Iteration 833, loss = 0.02208872\n","Iteration 834, loss = 0.02202047\n","Iteration 835, loss = 0.02195745\n","Iteration 836, loss = 0.02189932\n","Iteration 837, loss = 0.02184146\n","Iteration 838, loss = 0.02178833\n","Iteration 839, loss = 0.02172911\n","Iteration 840, loss = 0.02165535\n","Iteration 841, loss = 0.02159914\n","Iteration 842, loss = 0.02155860\n","Iteration 843, loss = 0.02147394\n","Iteration 844, loss = 0.02141540\n","Iteration 845, loss = 0.02136209\n","Iteration 846, loss = 0.02128136\n","Iteration 847, loss = 0.02122799\n","Iteration 848, loss = 0.02116275\n","Iteration 849, loss = 0.02108732\n","Iteration 850, loss = 0.02104707\n","Iteration 851, loss = 0.02096533\n","Iteration 852, loss = 0.02090895\n","Iteration 853, loss = 0.02085694\n","Iteration 854, loss = 0.02078746\n","Iteration 855, loss = 0.02071553\n","Iteration 856, loss = 0.02068889\n","Iteration 857, loss = 0.02060841\n","Iteration 858, loss = 0.02054386\n","Iteration 859, loss = 0.02048295\n","Iteration 860, loss = 0.02043001\n","Iteration 861, loss = 0.02037489\n","Iteration 862, loss = 0.02030917\n","Iteration 863, loss = 0.02024602\n","Iteration 864, loss = 0.02018604\n","Iteration 865, loss = 0.02012425\n","Iteration 866, loss = 0.02006597\n","Iteration 867, loss = 0.02002106\n","Iteration 868, loss = 0.01998990\n","Iteration 869, loss = 0.01988558\n","Iteration 870, loss = 0.01984501\n","Iteration 871, loss = 0.01976109\n","Iteration 872, loss = 0.01970382\n","Iteration 873, loss = 0.01965009\n","Iteration 874, loss = 0.01959386\n","Iteration 875, loss = 0.01953299\n","Iteration 876, loss = 0.01951501\n","Iteration 877, loss = 0.01944830\n","Iteration 878, loss = 0.01937188\n","Iteration 879, loss = 0.01929906\n","Iteration 880, loss = 0.01924714\n","Iteration 881, loss = 0.01919664\n","Iteration 882, loss = 0.01913116\n","Iteration 883, loss = 0.01908004\n","Iteration 884, loss = 0.01903419\n","Iteration 885, loss = 0.01897682\n","Iteration 886, loss = 0.01893489\n","Iteration 887, loss = 0.01885193\n","Iteration 888, loss = 0.01880620\n","Iteration 889, loss = 0.01874329\n","Iteration 890, loss = 0.01869152\n","Iteration 891, loss = 0.01862919\n","Iteration 892, loss = 0.01857541\n","Iteration 893, loss = 0.01853668\n","Iteration 894, loss = 0.01848141\n","Iteration 895, loss = 0.01841380\n","Iteration 896, loss = 0.01835982\n","Iteration 897, loss = 0.01830473\n","Iteration 898, loss = 0.01824819\n","Iteration 899, loss = 0.01819534\n","Iteration 900, loss = 0.01813995\n","Iteration 901, loss = 0.01811002\n","Iteration 902, loss = 0.01801922\n","Iteration 903, loss = 0.01798473\n","Iteration 904, loss = 0.01794313\n","Iteration 905, loss = 0.01787645\n","Iteration 906, loss = 0.01782428\n","Iteration 907, loss = 0.01777496\n","Iteration 908, loss = 0.01771830\n","Iteration 909, loss = 0.01767229\n","Iteration 910, loss = 0.01761203\n","Iteration 911, loss = 0.01756120\n","Iteration 912, loss = 0.01751276\n","Iteration 913, loss = 0.01747195\n","Iteration 914, loss = 0.01740256\n","Iteration 915, loss = 0.01734249\n","Iteration 916, loss = 0.01730111\n","Iteration 917, loss = 0.01726537\n","Iteration 918, loss = 0.01722685\n","Iteration 919, loss = 0.01719004\n","Iteration 920, loss = 0.01710455\n","Iteration 921, loss = 0.01706068\n","Iteration 922, loss = 0.01699673\n","Iteration 923, loss = 0.01697265\n","Iteration 924, loss = 0.01688614\n","Iteration 925, loss = 0.01685267\n","Iteration 926, loss = 0.01680207\n","Iteration 927, loss = 0.01673863\n","Iteration 928, loss = 0.01669276\n","Iteration 929, loss = 0.01663538\n","Iteration 930, loss = 0.01658325\n","Iteration 931, loss = 0.01655346\n","Iteration 932, loss = 0.01649702\n","Iteration 933, loss = 0.01647100\n","Iteration 934, loss = 0.01640374\n","Iteration 935, loss = 0.01639655\n","Iteration 936, loss = 0.01631794\n","Iteration 937, loss = 0.01625561\n","Iteration 938, loss = 0.01621392\n","Iteration 939, loss = 0.01615773\n","Iteration 940, loss = 0.01611462\n","Iteration 941, loss = 0.01607056\n","Iteration 942, loss = 0.01601277\n","Iteration 943, loss = 0.01595819\n","Iteration 944, loss = 0.01592396\n","Iteration 945, loss = 0.01588457\n","Iteration 946, loss = 0.01584663\n","Iteration 947, loss = 0.01580996\n","Iteration 948, loss = 0.01574623\n","Iteration 949, loss = 0.01570000\n","Iteration 950, loss = 0.01563853\n","Iteration 951, loss = 0.01560265\n","Iteration 952, loss = 0.01557565\n","Iteration 953, loss = 0.01549570\n","Iteration 954, loss = 0.01544769\n","Iteration 955, loss = 0.01539945\n","Iteration 956, loss = 0.01536357\n","Iteration 957, loss = 0.01531629\n","Iteration 958, loss = 0.01526455\n","Iteration 959, loss = 0.01523276\n","Iteration 960, loss = 0.01517517\n","Iteration 961, loss = 0.01512995\n","Iteration 962, loss = 0.01510285\n","Iteration 963, loss = 0.01506557\n","Iteration 964, loss = 0.01499992\n","Iteration 965, loss = 0.01495387\n","Iteration 966, loss = 0.01492693\n","Iteration 967, loss = 0.01485472\n","Iteration 968, loss = 0.01482643\n","Iteration 969, loss = 0.01477267\n","Iteration 970, loss = 0.01473306\n","Iteration 971, loss = 0.01470622\n","Iteration 972, loss = 0.01467056\n","Iteration 973, loss = 0.01461467\n","Iteration 974, loss = 0.01457138\n","Iteration 975, loss = 0.01453004\n","Iteration 976, loss = 0.01448857\n","Iteration 977, loss = 0.01443788\n","Iteration 978, loss = 0.01439614\n","Iteration 979, loss = 0.01436157\n","Iteration 980, loss = 0.01431751\n","Iteration 981, loss = 0.01426768\n","Iteration 982, loss = 0.01422684\n","Iteration 983, loss = 0.01422459\n","Iteration 984, loss = 0.01419600\n","Iteration 985, loss = 0.01411229\n","Iteration 986, loss = 0.01407580\n","Iteration 987, loss = 0.01402594\n","Iteration 988, loss = 0.01398689\n","Iteration 989, loss = 0.01396828\n","Iteration 990, loss = 0.01394900\n","Iteration 991, loss = 0.01389106\n","Iteration 992, loss = 0.01386962\n","Iteration 993, loss = 0.01380033\n","Iteration 994, loss = 0.01377023\n","Iteration 995, loss = 0.01372413\n","Iteration 996, loss = 0.01368820\n","Iteration 997, loss = 0.01364355\n","Iteration 998, loss = 0.01362863\n","Iteration 999, loss = 0.01356222\n","Iteration 1000, loss = 0.01351959\n","Iteration 1001, loss = 0.01349293\n","Iteration 1002, loss = 0.01344295\n","Iteration 1003, loss = 0.01341559\n","Iteration 1004, loss = 0.01337888\n","Iteration 1005, loss = 0.01336465\n","Iteration 1006, loss = 0.01330007\n","Iteration 1007, loss = 0.01327091\n","Iteration 1008, loss = 0.01323154\n","Iteration 1009, loss = 0.01318249\n","Iteration 1010, loss = 0.01315656\n","Iteration 1011, loss = 0.01311595\n","Iteration 1012, loss = 0.01307904\n","Iteration 1013, loss = 0.01303230\n","Iteration 1014, loss = 0.01299579\n","Iteration 1015, loss = 0.01299299\n","Iteration 1016, loss = 0.01293384\n","Iteration 1017, loss = 0.01288976\n","Iteration 1018, loss = 0.01288001\n","Iteration 1019, loss = 0.01281813\n","Iteration 1020, loss = 0.01279492\n","Iteration 1021, loss = 0.01276470\n","Iteration 1022, loss = 0.01272262\n","Iteration 1023, loss = 0.01268013\n","Iteration 1024, loss = 0.01264976\n","Iteration 1025, loss = 0.01261874\n","Iteration 1026, loss = 0.01257167\n","Iteration 1027, loss = 0.01254100\n","Iteration 1028, loss = 0.01249594\n","Iteration 1029, loss = 0.01246180\n","Iteration 1030, loss = 0.01242938\n","Iteration 1031, loss = 0.01238438\n","Iteration 1032, loss = 0.01235329\n","Iteration 1033, loss = 0.01233860\n","Iteration 1034, loss = 0.01230526\n","Iteration 1035, loss = 0.01227117\n","Iteration 1036, loss = 0.01221509\n","Iteration 1037, loss = 0.01218582\n","Iteration 1038, loss = 0.01215510\n","Iteration 1039, loss = 0.01211239\n","Iteration 1040, loss = 0.01207869\n","Iteration 1041, loss = 0.01205844\n","Iteration 1042, loss = 0.01200198\n","Iteration 1043, loss = 0.01197643\n","Iteration 1044, loss = 0.01195765\n","Iteration 1045, loss = 0.01190376\n","Iteration 1046, loss = 0.01187348\n","Iteration 1047, loss = 0.01184899\n","Iteration 1048, loss = 0.01180558\n","Iteration 1049, loss = 0.01177602\n","Iteration 1050, loss = 0.01173632\n","Iteration 1051, loss = 0.01172254\n","Iteration 1052, loss = 0.01169075\n","Iteration 1053, loss = 0.01164187\n","Iteration 1054, loss = 0.01161062\n","Iteration 1055, loss = 0.01157458\n","Iteration 1056, loss = 0.01154040\n","Iteration 1057, loss = 0.01152231\n","Iteration 1058, loss = 0.01148125\n","Iteration 1059, loss = 0.01144513\n","Iteration 1060, loss = 0.01141157\n","Iteration 1061, loss = 0.01139311\n","Iteration 1062, loss = 0.01137676\n","Iteration 1063, loss = 0.01131287\n","Iteration 1064, loss = 0.01129401\n","Iteration 1065, loss = 0.01124642\n","Iteration 1066, loss = 0.01123252\n","Iteration 1067, loss = 0.01122218\n","Iteration 1068, loss = 0.01117547\n","Iteration 1069, loss = 0.01113728\n","Iteration 1070, loss = 0.01110610\n","Iteration 1071, loss = 0.01108086\n","Iteration 1072, loss = 0.01105559\n","Iteration 1073, loss = 0.01100803\n","Iteration 1074, loss = 0.01097753\n","Iteration 1075, loss = 0.01094530\n","Iteration 1076, loss = 0.01091092\n","Iteration 1077, loss = 0.01088777\n","Iteration 1078, loss = 0.01085252\n","Iteration 1079, loss = 0.01083249\n","Iteration 1080, loss = 0.01079241\n","Iteration 1081, loss = 0.01075749\n","Iteration 1082, loss = 0.01073344\n","Iteration 1083, loss = 0.01071016\n","Iteration 1084, loss = 0.01066546\n","Iteration 1085, loss = 0.01064859\n","Iteration 1086, loss = 0.01063371\n","Iteration 1087, loss = 0.01059243\n","Iteration 1088, loss = 0.01056892\n","Iteration 1089, loss = 0.01053004\n","Iteration 1090, loss = 0.01049857\n","Iteration 1091, loss = 0.01048547\n","Iteration 1092, loss = 0.01043928\n","Iteration 1093, loss = 0.01043116\n","Iteration 1094, loss = 0.01040451\n","Iteration 1095, loss = 0.01036899\n","Iteration 1096, loss = 0.01033477\n","Iteration 1097, loss = 0.01030710\n","Iteration 1098, loss = 0.01027164\n","Iteration 1099, loss = 0.01024266\n","Iteration 1100, loss = 0.01021574\n","Iteration 1101, loss = 0.01018271\n","Iteration 1102, loss = 0.01016516\n","Iteration 1103, loss = 0.01013470\n","Iteration 1104, loss = 0.01009648\n","Iteration 1105, loss = 0.01006786\n","Iteration 1106, loss = 0.01005266\n","Iteration 1107, loss = 0.01001142\n","Iteration 1108, loss = 0.00998456\n","Iteration 1109, loss = 0.00996272\n","Iteration 1110, loss = 0.00993216\n","Iteration 1111, loss = 0.00989857\n","Iteration 1112, loss = 0.00988664\n","Iteration 1113, loss = 0.00986366\n","Iteration 1114, loss = 0.00983410\n","Iteration 1115, loss = 0.00979696\n","Iteration 1116, loss = 0.00977518\n","Iteration 1117, loss = 0.00975591\n","Iteration 1118, loss = 0.00972096\n","Iteration 1119, loss = 0.00970587\n","Iteration 1120, loss = 0.00964955\n","Iteration 1121, loss = 0.00965082\n","Iteration 1122, loss = 0.00960242\n","Iteration 1123, loss = 0.00959990\n","Iteration 1124, loss = 0.00956317\n","Iteration 1125, loss = 0.00953350\n","Iteration 1126, loss = 0.00950531\n","Iteration 1127, loss = 0.00948250\n","Iteration 1128, loss = 0.00943892\n","Iteration 1129, loss = 0.00942018\n","Iteration 1130, loss = 0.00938959\n","Iteration 1131, loss = 0.00937744\n","Iteration 1132, loss = 0.00935474\n","Iteration 1133, loss = 0.00932856\n","Iteration 1134, loss = 0.00930674\n","Iteration 1135, loss = 0.00927348\n","Iteration 1136, loss = 0.00924360\n","Iteration 1137, loss = 0.00924625\n","Iteration 1138, loss = 0.00918814\n","Iteration 1139, loss = 0.00916848\n","Iteration 1140, loss = 0.00914432\n","Iteration 1141, loss = 0.00912571\n","Iteration 1142, loss = 0.00910508\n","Iteration 1143, loss = 0.00907051\n","Iteration 1144, loss = 0.00905166\n","Iteration 1145, loss = 0.00901806\n","Iteration 1146, loss = 0.00900221\n","Iteration 1147, loss = 0.00897533\n","Iteration 1148, loss = 0.00894541\n","Iteration 1149, loss = 0.00891904\n","Iteration 1150, loss = 0.00889887\n","Iteration 1151, loss = 0.00888548\n","Iteration 1152, loss = 0.00885018\n","Iteration 1153, loss = 0.00883975\n","Iteration 1154, loss = 0.00881347\n","Iteration 1155, loss = 0.00879183\n","Iteration 1156, loss = 0.00875392\n","Iteration 1157, loss = 0.00874261\n","Iteration 1158, loss = 0.00871302\n","Iteration 1159, loss = 0.00868683\n","Iteration 1160, loss = 0.00866359\n","Iteration 1161, loss = 0.00864823\n","Iteration 1162, loss = 0.00863412\n","Iteration 1163, loss = 0.00859853\n","Iteration 1164, loss = 0.00858106\n","Iteration 1165, loss = 0.00854834\n","Iteration 1166, loss = 0.00852625\n","Iteration 1167, loss = 0.00849360\n","Iteration 1168, loss = 0.00847633\n","Iteration 1169, loss = 0.00844690\n","Iteration 1170, loss = 0.00842654\n","Iteration 1171, loss = 0.00841787\n","Iteration 1172, loss = 0.00838225\n","Iteration 1173, loss = 0.00837155\n","Iteration 1174, loss = 0.00835127\n","Iteration 1175, loss = 0.00832572\n","Iteration 1176, loss = 0.00830398\n","Iteration 1177, loss = 0.00826789\n","Iteration 1178, loss = 0.00824579\n","Iteration 1179, loss = 0.00822283\n","Iteration 1180, loss = 0.00820824\n","Iteration 1181, loss = 0.00818581\n","Iteration 1182, loss = 0.00816283\n","Iteration 1183, loss = 0.00814969\n","Iteration 1184, loss = 0.00813643\n","Iteration 1185, loss = 0.00809766\n","Iteration 1186, loss = 0.00807882\n","Iteration 1187, loss = 0.00808610\n","Iteration 1188, loss = 0.00802478\n","Iteration 1189, loss = 0.00802579\n","Iteration 1190, loss = 0.00798494\n","Iteration 1191, loss = 0.00797196\n","Iteration 1192, loss = 0.00794849\n","Iteration 1193, loss = 0.00795444\n","Iteration 1194, loss = 0.00790022\n","Iteration 1195, loss = 0.00788031\n","Iteration 1196, loss = 0.00786743\n","Iteration 1197, loss = 0.00784811\n","Iteration 1198, loss = 0.00781353\n","Iteration 1199, loss = 0.00779880\n","Iteration 1200, loss = 0.00779848\n","Iteration 1201, loss = 0.00777231\n","Iteration 1202, loss = 0.00775320\n","Iteration 1203, loss = 0.00771346\n","Iteration 1204, loss = 0.00771128\n","Iteration 1205, loss = 0.00767459\n","Iteration 1206, loss = 0.00766371\n","Iteration 1207, loss = 0.00764619\n","Iteration 1208, loss = 0.00762608\n","Iteration 1209, loss = 0.00760282\n","Iteration 1210, loss = 0.00757685\n","Iteration 1211, loss = 0.00756584\n","Iteration 1212, loss = 0.00753943\n","Iteration 1213, loss = 0.00753872\n","Iteration 1214, loss = 0.00751239\n","Iteration 1215, loss = 0.00748606\n","Iteration 1216, loss = 0.00745856\n","Iteration 1217, loss = 0.00747082\n","Iteration 1218, loss = 0.00744323\n","Iteration 1219, loss = 0.00740516\n","Iteration 1220, loss = 0.00738003\n","Iteration 1221, loss = 0.00735768\n","Iteration 1222, loss = 0.00734192\n","Iteration 1223, loss = 0.00731425\n","Iteration 1224, loss = 0.00729660\n","Iteration 1225, loss = 0.00728833\n","Iteration 1226, loss = 0.00727412\n","Iteration 1227, loss = 0.00724288\n","Iteration 1228, loss = 0.00723355\n","Iteration 1229, loss = 0.00720966\n","Iteration 1230, loss = 0.00719921\n","Iteration 1231, loss = 0.00715300\n","Iteration 1232, loss = 0.00716640\n","Iteration 1233, loss = 0.00714244\n","Iteration 1234, loss = 0.00712336\n","Iteration 1235, loss = 0.00708538\n","Iteration 1236, loss = 0.00707713\n","Iteration 1237, loss = 0.00705782\n","Iteration 1238, loss = 0.00704312\n","Iteration 1239, loss = 0.00702592\n","Iteration 1240, loss = 0.00701401\n","Iteration 1241, loss = 0.00698610\n","Iteration 1242, loss = 0.00696309\n","Iteration 1243, loss = 0.00695218\n","Iteration 1244, loss = 0.00693464\n","Iteration 1245, loss = 0.00693099\n","Iteration 1246, loss = 0.00688244\n","Iteration 1247, loss = 0.00687517\n","Iteration 1248, loss = 0.00687078\n","Iteration 1249, loss = 0.00683376\n","Iteration 1250, loss = 0.00684654\n","Iteration 1251, loss = 0.00681616\n","Iteration 1252, loss = 0.00677336\n","Iteration 1253, loss = 0.00676593\n","Iteration 1254, loss = 0.00675864\n","Iteration 1255, loss = 0.00673285\n","Iteration 1256, loss = 0.00671537\n","Iteration 1257, loss = 0.00669662\n","Iteration 1258, loss = 0.00668081\n","Iteration 1259, loss = 0.00667315\n","Iteration 1260, loss = 0.00666840\n","Iteration 1261, loss = 0.00662710\n","Iteration 1262, loss = 0.00661014\n","Iteration 1263, loss = 0.00658548\n","Iteration 1264, loss = 0.00658714\n","Iteration 1265, loss = 0.00655723\n","Iteration 1266, loss = 0.00654355\n","Iteration 1267, loss = 0.00653970\n","Iteration 1268, loss = 0.00653254\n","Iteration 1269, loss = 0.00650651\n","Iteration 1270, loss = 0.00648123\n","Iteration 1271, loss = 0.00646290\n","Iteration 1272, loss = 0.00643701\n","Iteration 1273, loss = 0.00642460\n","Iteration 1274, loss = 0.00640356\n","Iteration 1275, loss = 0.00638497\n","Iteration 1276, loss = 0.00636378\n","Iteration 1277, loss = 0.00636329\n","Iteration 1278, loss = 0.00636014\n","Iteration 1279, loss = 0.00632464\n","Iteration 1280, loss = 0.00631414\n","Iteration 1281, loss = 0.00628281\n","Iteration 1282, loss = 0.00626818\n","Iteration 1283, loss = 0.00626839\n","Iteration 1284, loss = 0.00627412\n","Iteration 1285, loss = 0.00623785\n","Iteration 1286, loss = 0.00622648\n","Iteration 1287, loss = 0.00620481\n","Iteration 1288, loss = 0.00618034\n","Iteration 1289, loss = 0.00616642\n","Iteration 1290, loss = 0.00614578\n","Iteration 1291, loss = 0.00612555\n","Iteration 1292, loss = 0.00612547\n","Iteration 1293, loss = 0.00611477\n","Iteration 1294, loss = 0.00611912\n","Iteration 1295, loss = 0.00606605\n","Iteration 1296, loss = 0.00606076\n","Iteration 1297, loss = 0.00603341\n","Iteration 1298, loss = 0.00602628\n","Iteration 1299, loss = 0.00601369\n","Iteration 1300, loss = 0.00600678\n","Iteration 1301, loss = 0.00599133\n","Iteration 1302, loss = 0.00598162\n","Iteration 1303, loss = 0.00596279\n","Iteration 1304, loss = 0.00592512\n","Iteration 1305, loss = 0.00592796\n","Iteration 1306, loss = 0.00592308\n","Iteration 1307, loss = 0.00590224\n","Iteration 1308, loss = 0.00587946\n","Iteration 1309, loss = 0.00588422\n","Iteration 1310, loss = 0.00587573\n","Iteration 1311, loss = 0.00582230\n","Iteration 1312, loss = 0.00582891\n","Iteration 1313, loss = 0.00579427\n","Iteration 1314, loss = 0.00577848\n","Iteration 1315, loss = 0.00578630\n","Iteration 1316, loss = 0.00576451\n","Iteration 1317, loss = 0.00575155\n","Iteration 1318, loss = 0.00572814\n","Iteration 1319, loss = 0.00572421\n","Iteration 1320, loss = 0.00570521\n","Iteration 1321, loss = 0.00568518\n","Iteration 1322, loss = 0.00568255\n","Iteration 1323, loss = 0.00565773\n","Iteration 1324, loss = 0.00564331\n","Iteration 1325, loss = 0.00563968\n","Iteration 1326, loss = 0.00562457\n","Iteration 1327, loss = 0.00561200\n","Iteration 1328, loss = 0.00562297\n","Iteration 1329, loss = 0.00559418\n","Iteration 1330, loss = 0.00556343\n","Iteration 1331, loss = 0.00555050\n","Iteration 1332, loss = 0.00553085\n","Iteration 1333, loss = 0.00551065\n","Iteration 1334, loss = 0.00551870\n","Iteration 1335, loss = 0.00550416\n","Iteration 1336, loss = 0.00548499\n","Iteration 1337, loss = 0.00547017\n","Iteration 1338, loss = 0.00545591\n","Iteration 1339, loss = 0.00544819\n","Iteration 1340, loss = 0.00543939\n","Iteration 1341, loss = 0.00542651\n","Iteration 1342, loss = 0.00540392\n","Iteration 1343, loss = 0.00541114\n","Iteration 1344, loss = 0.00535388\n","Iteration 1345, loss = 0.00534071\n","Iteration 1346, loss = 0.00533186\n","Iteration 1347, loss = 0.00532915\n","Iteration 1348, loss = 0.00531908\n","Iteration 1349, loss = 0.00530234\n","Iteration 1350, loss = 0.00529105\n","Iteration 1351, loss = 0.00527550\n","Iteration 1352, loss = 0.00527509\n","Iteration 1353, loss = 0.00525097\n","Iteration 1354, loss = 0.00524080\n","Iteration 1355, loss = 0.00524738\n","Iteration 1356, loss = 0.00522211\n","Iteration 1357, loss = 0.00519594\n","Iteration 1358, loss = 0.00519559\n","Iteration 1359, loss = 0.00517178\n","Iteration 1360, loss = 0.00515810\n","Iteration 1361, loss = 0.00515886\n","Iteration 1362, loss = 0.00514251\n","Iteration 1363, loss = 0.00512706\n","Iteration 1364, loss = 0.00511171\n","Iteration 1365, loss = 0.00510503\n","Iteration 1366, loss = 0.00510479\n","Iteration 1367, loss = 0.00507686\n","Iteration 1368, loss = 0.00505838\n","Iteration 1369, loss = 0.00505209\n","Iteration 1370, loss = 0.00505708\n","Iteration 1371, loss = 0.00503309\n","Iteration 1372, loss = 0.00500131\n","Iteration 1373, loss = 0.00500321\n","Iteration 1374, loss = 0.00498322\n","Iteration 1375, loss = 0.00497481\n","Iteration 1376, loss = 0.00496124\n","Iteration 1377, loss = 0.00497081\n","Iteration 1378, loss = 0.00493714\n","Iteration 1379, loss = 0.00494441\n","Iteration 1380, loss = 0.00493835\n","Iteration 1381, loss = 0.00491250\n","Iteration 1382, loss = 0.00489982\n","Iteration 1383, loss = 0.00487631\n","Iteration 1384, loss = 0.00487357\n","Iteration 1385, loss = 0.00485734\n","Iteration 1386, loss = 0.00484622\n","Iteration 1387, loss = 0.00483316\n","Iteration 1388, loss = 0.00486458\n","Iteration 1389, loss = 0.00483455\n","Iteration 1390, loss = 0.00480166\n","Iteration 1391, loss = 0.00480855\n","Iteration 1392, loss = 0.00476960\n","Iteration 1393, loss = 0.00477013\n","Iteration 1394, loss = 0.00475218\n","Iteration 1395, loss = 0.00474882\n","Iteration 1396, loss = 0.00474689\n","Iteration 1397, loss = 0.00472199\n","Iteration 1398, loss = 0.00470879\n","Iteration 1399, loss = 0.00469016\n","Iteration 1400, loss = 0.00468892\n","Iteration 1401, loss = 0.00470205\n","Iteration 1402, loss = 0.00466194\n","Iteration 1403, loss = 0.00465023\n","Iteration 1404, loss = 0.00466818\n","Iteration 1405, loss = 0.00464027\n","Iteration 1406, loss = 0.00463008\n","Iteration 1407, loss = 0.00461034\n","Iteration 1408, loss = 0.00460143\n","Iteration 1409, loss = 0.00462514\n","Iteration 1410, loss = 0.00459380\n","Iteration 1411, loss = 0.00457149\n","Iteration 1412, loss = 0.00455756\n","Iteration 1413, loss = 0.00456040\n","Iteration 1414, loss = 0.00454765\n","Iteration 1415, loss = 0.00453860\n","Iteration 1416, loss = 0.00452270\n","Iteration 1417, loss = 0.00450584\n","Iteration 1418, loss = 0.00450919\n","Iteration 1419, loss = 0.00449185\n","Iteration 1420, loss = 0.00448495\n","Iteration 1421, loss = 0.00450718\n","Iteration 1422, loss = 0.00444939\n","Iteration 1423, loss = 0.00447602\n","Iteration 1424, loss = 0.00446271\n","Iteration 1425, loss = 0.00443347\n","Iteration 1426, loss = 0.00441746\n","Iteration 1427, loss = 0.00441002\n","Iteration 1428, loss = 0.00443339\n","Iteration 1429, loss = 0.00438770\n","Iteration 1430, loss = 0.00438064\n","Iteration 1431, loss = 0.00436745\n","Iteration 1432, loss = 0.00435185\n","Iteration 1433, loss = 0.00435227\n","Iteration 1434, loss = 0.00433928\n","Iteration 1435, loss = 0.00431765\n","Iteration 1436, loss = 0.00431040\n","Iteration 1437, loss = 0.00430506\n","Iteration 1438, loss = 0.00433290\n","Iteration 1439, loss = 0.00428023\n","Iteration 1440, loss = 0.00426721\n","Iteration 1441, loss = 0.00427812\n","Iteration 1442, loss = 0.00425967\n","Iteration 1443, loss = 0.00424333\n","Iteration 1444, loss = 0.00423915\n","Iteration 1445, loss = 0.00422573\n","Iteration 1446, loss = 0.00420589\n","Iteration 1447, loss = 0.00421546\n","Iteration 1448, loss = 0.00419497\n","Iteration 1449, loss = 0.00418128\n","Iteration 1450, loss = 0.00418887\n","Iteration 1451, loss = 0.00417167\n","Iteration 1452, loss = 0.00417105\n","Iteration 1453, loss = 0.00415932\n","Iteration 1454, loss = 0.00414517\n","Iteration 1455, loss = 0.00413325\n","Iteration 1456, loss = 0.00413240\n","Iteration 1457, loss = 0.00411086\n","Iteration 1458, loss = 0.00410355\n","Iteration 1459, loss = 0.00408613\n","Iteration 1460, loss = 0.00410077\n","Iteration 1461, loss = 0.00407982\n","Iteration 1462, loss = 0.00407233\n","Iteration 1463, loss = 0.00404577\n","Iteration 1464, loss = 0.00405674\n","Iteration 1465, loss = 0.00403202\n","Iteration 1466, loss = 0.00403426\n","Iteration 1467, loss = 0.00405218\n","Iteration 1468, loss = 0.00403085\n","Iteration 1469, loss = 0.00401175\n","Iteration 1470, loss = 0.00400790\n","Iteration 1471, loss = 0.00397734\n","Iteration 1472, loss = 0.00396618\n","Iteration 1473, loss = 0.00397448\n","Iteration 1474, loss = 0.00395675\n","Iteration 1475, loss = 0.00394770\n","Iteration 1476, loss = 0.00393299\n","Iteration 1477, loss = 0.00393577\n","Iteration 1478, loss = 0.00391839\n","Iteration 1479, loss = 0.00393302\n","Iteration 1480, loss = 0.00392211\n","Iteration 1481, loss = 0.00390645\n","Iteration 1482, loss = 0.00389310\n","Iteration 1483, loss = 0.00388212\n","Iteration 1484, loss = 0.00387055\n","Iteration 1485, loss = 0.00385944\n","Iteration 1486, loss = 0.00385439\n","Iteration 1487, loss = 0.00383831\n","Iteration 1488, loss = 0.00384211\n","Iteration 1489, loss = 0.00382529\n","Iteration 1490, loss = 0.00383861\n","Iteration 1491, loss = 0.00381454\n","Iteration 1492, loss = 0.00379570\n","Iteration 1493, loss = 0.00379861\n","Iteration 1494, loss = 0.00378036\n","Iteration 1495, loss = 0.00378366\n","Iteration 1496, loss = 0.00377655\n","Iteration 1497, loss = 0.00378023\n","Iteration 1498, loss = 0.00375966\n","Iteration 1499, loss = 0.00375962\n","Iteration 1500, loss = 0.00375057\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-05, verbose=True)"],"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-05, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-05, verbose=True)</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["previsoes_credit = rede_neural_credit.predict(x_credit_teste)\n","previsoes_credit"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0RTqYHlvK3Up","executionInfo":{"status":"ok","timestamp":1705957453340,"user_tz":180,"elapsed":8,"user":{"displayName":"Kaique F. Santos","userId":"18416165402555736361"}},"outputId":"40607780-46ec-40dd-bb51-9806b9fbd668"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n","       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n","       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n","       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n","       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n","       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n","       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n","       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n","       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# Métricas\n","from sklearn.metrics import accuracy_score, classification_report\n","from yellowbrick.classifier import ConfusionMatrix"],"metadata":{"id":"hzTHBLZELAsB","executionInfo":{"status":"ok","timestamp":1705957453792,"user_tz":180,"elapsed":458,"user":{"displayName":"Kaique F. Santos","userId":"18416165402555736361"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["print(f'A taxa de acerto é de: {accuracy_score(y_credit_teste, previsoes_credit)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NNojE_JLLX1s","executionInfo":{"status":"ok","timestamp":1705957453792,"user_tz":180,"elapsed":10,"user":{"displayName":"Kaique F. Santos","userId":"18416165402555736361"}},"outputId":"c4a230e3-f19d-4de3-a0f9-3d62e50e5af2"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["A taxa de acerto é de: 0.998\n"]}]},{"cell_type":"code","source":["cm = ConfusionMatrix(rede_neural_credit)\n","cm.fit(x_credit_treinamento, y_credit_treinamento)\n","cm.score(x_credit_teste, y_credit_teste)\n","\n","# 435, 1= Pessoa que paga classificada que não paga o empréstimo\n","# 0= Classificadas corretamente , 64"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":496},"id":"uJHoFZi3MSsI","executionInfo":{"status":"ok","timestamp":1705957453792,"user_tz":180,"elapsed":8,"user":{"displayName":"Kaique F. Santos","userId":"18416165402555736361"}},"outputId":"1cffbaa1-84e0-46d3-8311-790e45606407"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.998"]},"metadata":{},"execution_count":10},{"output_type":"display_data","data":{"text/plain":["<Figure size 800x550 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["print(f'Tabela de classificação: \\n{classification_report(y_credit_teste, previsoes_credit)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pXbnTJ3DMiyz","executionInfo":{"status":"ok","timestamp":1705957453792,"user_tz":180,"elapsed":6,"user":{"displayName":"Kaique F. Santos","userId":"18416165402555736361"}},"outputId":"93387059-18f1-460d-bcde-2f09d21e8dfc"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Tabela de classificação: \n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00       436\n","           1       0.98      1.00      0.99        64\n","\n","    accuracy                           1.00       500\n","   macro avg       0.99      1.00      1.00       500\n","weighted avg       1.00      1.00      1.00       500\n","\n"]}]},{"cell_type":"markdown","source":["# Base Census"],"metadata":{"id":"QA-5wWauND-K"}},{"cell_type":"code","source":["# importando as variáveis\n","\n","with open('/content/drive/MyDrive/variáveis de teste e treinamento(credit, census)/census.pkl', 'rb') as f:\n","    x_census_treinamento, y_census_treinamento, x_census_teste, y_census_teste = pickle.load(f)"],"metadata":{"id":"X7v2E38WNFjk","executionInfo":{"status":"ok","timestamp":1705957455793,"user_tz":180,"elapsed":2006,"user":{"displayName":"Kaique F. Santos","userId":"18416165402555736361"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Fazendo shape das variáveis\n","\n","print(x_census_treinamento.shape, y_census_treinamento.shape)\n","print(x_census_teste.shape, y_census_teste.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3BdR5Tft--5J","executionInfo":{"status":"ok","timestamp":1705957455793,"user_tz":180,"elapsed":3,"user":{"displayName":"Kaique F. Santos","userId":"18416165402555736361"}},"outputId":"89675f54-388f-48ef-a4fb-a7c01c34ac4d"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["(27676, 108) (27676,)\n","(4885, 108) (4885,)\n"]}]},{"cell_type":"code","source":["108 + 9"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O38xNtXOAfxS","executionInfo":{"status":"ok","timestamp":1705957823475,"user_tz":180,"elapsed":3,"user":{"displayName":"Kaique F. Santos","userId":"18416165402555736361"}},"outputId":"1429a77b-6f8f-4f40-d803-078c6c24650d"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["117"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["# Criando rede neural\n","# 108 -> 55 -> 55 -> 1\n","rede_neural_census = MLPClassifier(max_iter=1000, verbose=True, solver='adam', activation='relu', hidden_layer_sizes=(55, 55), tol=0.0000100)\n","rede_neural_census.fit(x_census_treinamento, y_census_treinamento)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"R4bH9aWV_W3I","executionInfo":{"status":"ok","timestamp":1705957822573,"user_tz":180,"elapsed":70631,"user":{"displayName":"Kaique F. Santos","userId":"18416165402555736361"}},"outputId":"13369ea5-eaed-49b0-c286-0beb8b5b3751"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.40873091\n","Iteration 2, loss = 0.32832281\n","Iteration 3, loss = 0.31670477\n","Iteration 4, loss = 0.30820366\n","Iteration 5, loss = 0.30281535\n","Iteration 6, loss = 0.29906658\n","Iteration 7, loss = 0.29584256\n","Iteration 8, loss = 0.29306932\n","Iteration 9, loss = 0.29110249\n","Iteration 10, loss = 0.28861122\n","Iteration 11, loss = 0.28586868\n","Iteration 12, loss = 0.28461859\n","Iteration 13, loss = 0.28176020\n","Iteration 14, loss = 0.28094063\n","Iteration 15, loss = 0.27849757\n","Iteration 16, loss = 0.27695615\n","Iteration 17, loss = 0.27479075\n","Iteration 18, loss = 0.27331076\n","Iteration 19, loss = 0.27193659\n","Iteration 20, loss = 0.27021650\n","Iteration 21, loss = 0.26857151\n","Iteration 22, loss = 0.26672454\n","Iteration 23, loss = 0.26488187\n","Iteration 24, loss = 0.26326842\n","Iteration 25, loss = 0.26221803\n","Iteration 26, loss = 0.26097024\n","Iteration 27, loss = 0.25910400\n","Iteration 28, loss = 0.25746124\n","Iteration 29, loss = 0.25711643\n","Iteration 30, loss = 0.25521149\n","Iteration 31, loss = 0.25360586\n","Iteration 32, loss = 0.25195171\n","Iteration 33, loss = 0.25117248\n","Iteration 34, loss = 0.24932818\n","Iteration 35, loss = 0.24797392\n","Iteration 36, loss = 0.24654847\n","Iteration 37, loss = 0.24504625\n","Iteration 38, loss = 0.24486355\n","Iteration 39, loss = 0.24272587\n","Iteration 40, loss = 0.24305889\n","Iteration 41, loss = 0.24045548\n","Iteration 42, loss = 0.23950720\n","Iteration 43, loss = 0.23819438\n","Iteration 44, loss = 0.23711421\n","Iteration 45, loss = 0.23475942\n","Iteration 46, loss = 0.23438675\n","Iteration 47, loss = 0.23340821\n","Iteration 48, loss = 0.23373935\n","Iteration 49, loss = 0.23161590\n","Iteration 50, loss = 0.23061363\n","Iteration 51, loss = 0.22932046\n","Iteration 52, loss = 0.22878642\n","Iteration 53, loss = 0.22820200\n","Iteration 54, loss = 0.22638236\n","Iteration 55, loss = 0.22577150\n","Iteration 56, loss = 0.22561030\n","Iteration 57, loss = 0.22437290\n","Iteration 58, loss = 0.22369334\n","Iteration 59, loss = 0.22206972\n","Iteration 60, loss = 0.22142289\n","Iteration 61, loss = 0.22003338\n","Iteration 62, loss = 0.21879667\n","Iteration 63, loss = 0.21944699\n","Iteration 64, loss = 0.21798892\n","Iteration 65, loss = 0.21801095\n","Iteration 66, loss = 0.21668933\n","Iteration 67, loss = 0.21653404\n","Iteration 68, loss = 0.21492103\n","Iteration 69, loss = 0.21312356\n","Iteration 70, loss = 0.21311638\n","Iteration 71, loss = 0.21302052\n","Iteration 72, loss = 0.21230910\n","Iteration 73, loss = 0.21256232\n","Iteration 74, loss = 0.20991605\n","Iteration 75, loss = 0.20931302\n","Iteration 76, loss = 0.20854800\n","Iteration 77, loss = 0.20832774\n","Iteration 78, loss = 0.20746944\n","Iteration 79, loss = 0.20699072\n","Iteration 80, loss = 0.20643792\n","Iteration 81, loss = 0.20634924\n","Iteration 82, loss = 0.20472808\n","Iteration 83, loss = 0.20399169\n","Iteration 84, loss = 0.20382127\n","Iteration 85, loss = 0.20446958\n","Iteration 86, loss = 0.20439059\n","Iteration 87, loss = 0.20259693\n","Iteration 88, loss = 0.20222676\n","Iteration 89, loss = 0.20116872\n","Iteration 90, loss = 0.20162145\n","Iteration 91, loss = 0.19929174\n","Iteration 92, loss = 0.20063707\n","Iteration 93, loss = 0.19948837\n","Iteration 94, loss = 0.19999449\n","Iteration 95, loss = 0.19887562\n","Iteration 96, loss = 0.19742775\n","Iteration 97, loss = 0.19763186\n","Iteration 98, loss = 0.19721708\n","Iteration 99, loss = 0.19668266\n","Iteration 100, loss = 0.19546004\n","Iteration 101, loss = 0.19583132\n","Iteration 102, loss = 0.19356454\n","Iteration 103, loss = 0.19363520\n","Iteration 104, loss = 0.19383431\n","Iteration 105, loss = 0.19285726\n","Iteration 106, loss = 0.19184391\n","Iteration 107, loss = 0.19281564\n","Iteration 108, loss = 0.19081076\n","Iteration 109, loss = 0.19101227\n","Iteration 110, loss = 0.19080415\n","Iteration 111, loss = 0.19106347\n","Iteration 112, loss = 0.19101818\n","Iteration 113, loss = 0.18938436\n","Iteration 114, loss = 0.18895070\n","Iteration 115, loss = 0.18851598\n","Iteration 116, loss = 0.18774912\n","Iteration 117, loss = 0.18661525\n","Iteration 118, loss = 0.18683892\n","Iteration 119, loss = 0.18825095\n","Iteration 120, loss = 0.18623845\n","Iteration 121, loss = 0.18611369\n","Iteration 122, loss = 0.18525341\n","Iteration 123, loss = 0.18323853\n","Iteration 124, loss = 0.18403608\n","Iteration 125, loss = 0.18436187\n","Iteration 126, loss = 0.18338774\n","Iteration 127, loss = 0.18426390\n","Iteration 128, loss = 0.18517384\n","Iteration 129, loss = 0.18264392\n","Iteration 130, loss = 0.18259917\n","Iteration 131, loss = 0.18496721\n","Iteration 132, loss = 0.18154455\n","Iteration 133, loss = 0.18147706\n","Iteration 134, loss = 0.18056831\n","Iteration 135, loss = 0.18090181\n","Iteration 136, loss = 0.18040064\n","Iteration 137, loss = 0.18079425\n","Iteration 138, loss = 0.17890569\n","Iteration 139, loss = 0.17885746\n","Iteration 140, loss = 0.17943648\n","Iteration 141, loss = 0.17787713\n","Iteration 142, loss = 0.17733881\n","Iteration 143, loss = 0.17704326\n","Iteration 144, loss = 0.17762223\n","Iteration 145, loss = 0.17899689\n","Iteration 146, loss = 0.17728493\n","Iteration 147, loss = 0.17650524\n","Iteration 148, loss = 0.17651442\n","Iteration 149, loss = 0.17593848\n","Iteration 150, loss = 0.17496058\n","Iteration 151, loss = 0.17535698\n","Iteration 152, loss = 0.17529796\n","Iteration 153, loss = 0.17402439\n","Iteration 154, loss = 0.17468833\n","Iteration 155, loss = 0.17494420\n","Iteration 156, loss = 0.17375373\n","Iteration 157, loss = 0.17341084\n","Iteration 158, loss = 0.17250165\n","Iteration 159, loss = 0.17185292\n","Iteration 160, loss = 0.17242373\n","Iteration 161, loss = 0.17230270\n","Iteration 162, loss = 0.17271129\n","Iteration 163, loss = 0.17129328\n","Iteration 164, loss = 0.17136021\n","Iteration 165, loss = 0.17093960\n","Iteration 166, loss = 0.17111985\n","Iteration 167, loss = 0.17061112\n","Iteration 168, loss = 0.17014461\n","Iteration 169, loss = 0.17210048\n","Iteration 170, loss = 0.17026366\n","Iteration 171, loss = 0.16921254\n","Iteration 172, loss = 0.16845136\n","Iteration 173, loss = 0.17060448\n","Iteration 174, loss = 0.16924830\n","Iteration 175, loss = 0.16842966\n","Iteration 176, loss = 0.16774223\n","Iteration 177, loss = 0.16866823\n","Iteration 178, loss = 0.16725305\n","Iteration 179, loss = 0.16805619\n","Iteration 180, loss = 0.16748312\n","Iteration 181, loss = 0.16675738\n","Iteration 182, loss = 0.16659136\n","Iteration 183, loss = 0.16647667\n","Iteration 184, loss = 0.16644355\n","Iteration 185, loss = 0.16623686\n","Iteration 186, loss = 0.16550245\n","Iteration 187, loss = 0.16603778\n","Iteration 188, loss = 0.16491136\n","Iteration 189, loss = 0.16419257\n","Iteration 190, loss = 0.16407551\n","Iteration 191, loss = 0.16319667\n","Iteration 192, loss = 0.16435955\n","Iteration 193, loss = 0.16502216\n","Iteration 194, loss = 0.16330394\n","Iteration 195, loss = 0.16373395\n","Iteration 196, loss = 0.16399726\n","Iteration 197, loss = 0.16215341\n","Iteration 198, loss = 0.16287595\n","Iteration 199, loss = 0.16184480\n","Iteration 200, loss = 0.16295277\n","Iteration 201, loss = 0.16273639\n","Iteration 202, loss = 0.16148453\n","Iteration 203, loss = 0.16240721\n","Iteration 204, loss = 0.16171844\n","Iteration 205, loss = 0.16158774\n","Iteration 206, loss = 0.16206428\n","Iteration 207, loss = 0.16214188\n","Iteration 208, loss = 0.15997983\n","Iteration 209, loss = 0.16050352\n","Iteration 210, loss = 0.15996171\n","Iteration 211, loss = 0.15896195\n","Iteration 212, loss = 0.16039603\n","Iteration 213, loss = 0.16145345\n","Iteration 214, loss = 0.16006021\n","Iteration 215, loss = 0.15993230\n","Iteration 216, loss = 0.15835518\n","Iteration 217, loss = 0.15834260\n","Iteration 218, loss = 0.16013214\n","Iteration 219, loss = 0.15733180\n","Iteration 220, loss = 0.15892858\n","Iteration 221, loss = 0.15734645\n","Iteration 222, loss = 0.15808090\n","Iteration 223, loss = 0.15881663\n","Iteration 224, loss = 0.15785682\n","Iteration 225, loss = 0.15782860\n","Iteration 226, loss = 0.15745477\n","Iteration 227, loss = 0.15785914\n","Iteration 228, loss = 0.15570476\n","Iteration 229, loss = 0.15565365\n","Iteration 230, loss = 0.15691599\n","Iteration 231, loss = 0.15723093\n","Iteration 232, loss = 0.15447231\n","Iteration 233, loss = 0.15560955\n","Iteration 234, loss = 0.15459863\n","Iteration 235, loss = 0.15593194\n","Iteration 236, loss = 0.15519498\n","Iteration 237, loss = 0.15551934\n","Iteration 238, loss = 0.15544981\n","Iteration 239, loss = 0.15301261\n","Iteration 240, loss = 0.15452701\n","Iteration 241, loss = 0.15404934\n","Iteration 242, loss = 0.15530630\n","Iteration 243, loss = 0.15387708\n","Iteration 244, loss = 0.15394984\n","Iteration 245, loss = 0.15367779\n","Iteration 246, loss = 0.15288537\n","Iteration 247, loss = 0.15328987\n","Iteration 248, loss = 0.15448836\n","Iteration 249, loss = 0.15308000\n","Iteration 250, loss = 0.15325081\n","Iteration 251, loss = 0.15144992\n","Iteration 252, loss = 0.15368881\n","Iteration 253, loss = 0.15333832\n","Iteration 254, loss = 0.15179446\n","Iteration 255, loss = 0.15096165\n","Iteration 256, loss = 0.15109726\n","Iteration 257, loss = 0.15035990\n","Iteration 258, loss = 0.15119807\n","Iteration 259, loss = 0.15103388\n","Iteration 260, loss = 0.15236520\n","Iteration 261, loss = 0.15067428\n","Iteration 262, loss = 0.15221018\n","Iteration 263, loss = 0.15060530\n","Iteration 264, loss = 0.15100809\n","Iteration 265, loss = 0.14849398\n","Iteration 266, loss = 0.15039345\n","Iteration 267, loss = 0.14851666\n","Iteration 268, loss = 0.14985114\n","Iteration 269, loss = 0.14864342\n","Iteration 270, loss = 0.15025214\n","Iteration 271, loss = 0.14795512\n","Iteration 272, loss = 0.14783540\n","Iteration 273, loss = 0.14955470\n","Iteration 274, loss = 0.14808743\n","Iteration 275, loss = 0.14713901\n","Iteration 276, loss = 0.14868877\n","Iteration 277, loss = 0.14772762\n","Iteration 278, loss = 0.14801696\n","Iteration 279, loss = 0.14865138\n","Iteration 280, loss = 0.14698522\n","Iteration 281, loss = 0.14654324\n","Iteration 282, loss = 0.14693947\n","Iteration 283, loss = 0.14717783\n","Iteration 284, loss = 0.14823510\n","Iteration 285, loss = 0.14879261\n","Iteration 286, loss = 0.14688641\n","Iteration 287, loss = 0.14581960\n","Iteration 288, loss = 0.14723788\n","Iteration 289, loss = 0.14710605\n","Iteration 290, loss = 0.14614575\n","Iteration 291, loss = 0.14436952\n","Iteration 292, loss = 0.14664922\n","Iteration 293, loss = 0.14626274\n","Iteration 294, loss = 0.14565185\n","Iteration 295, loss = 0.14522633\n","Iteration 296, loss = 0.14482952\n","Iteration 297, loss = 0.14574157\n","Iteration 298, loss = 0.14434438\n","Iteration 299, loss = 0.14355828\n","Iteration 300, loss = 0.14365245\n","Iteration 301, loss = 0.14497366\n","Iteration 302, loss = 0.14590489\n","Iteration 303, loss = 0.14657776\n","Iteration 304, loss = 0.14683938\n","Iteration 305, loss = 0.14459690\n","Iteration 306, loss = 0.14517554\n","Iteration 307, loss = 0.14417570\n","Iteration 308, loss = 0.14278226\n","Iteration 309, loss = 0.14513050\n","Iteration 310, loss = 0.14389794\n","Iteration 311, loss = 0.14319905\n","Iteration 312, loss = 0.14332678\n","Iteration 313, loss = 0.14212217\n","Iteration 314, loss = 0.14466699\n","Iteration 315, loss = 0.14168089\n","Iteration 316, loss = 0.14164107\n","Iteration 317, loss = 0.14269633\n","Iteration 318, loss = 0.14191565\n","Iteration 319, loss = 0.14217295\n","Iteration 320, loss = 0.14198352\n","Iteration 321, loss = 0.14228632\n","Iteration 322, loss = 0.14116389\n","Iteration 323, loss = 0.14147363\n","Iteration 324, loss = 0.14219373\n","Iteration 325, loss = 0.14064293\n","Iteration 326, loss = 0.14084675\n","Iteration 327, loss = 0.14214839\n","Iteration 328, loss = 0.14098671\n","Iteration 329, loss = 0.14196416\n","Iteration 330, loss = 0.13860009\n","Iteration 331, loss = 0.14068715\n","Iteration 332, loss = 0.14031121\n","Iteration 333, loss = 0.14234051\n","Iteration 334, loss = 0.14113527\n","Iteration 335, loss = 0.13951978\n","Iteration 336, loss = 0.14008861\n","Iteration 337, loss = 0.14032562\n","Iteration 338, loss = 0.14007242\n","Iteration 339, loss = 0.13974912\n","Iteration 340, loss = 0.13893385\n","Iteration 341, loss = 0.13973860\n","Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"]},{"output_type":"execute_result","data":{"text/plain":["MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1000, tol=1e-05,\n","              verbose=True)"],"text/html":["<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1000, tol=1e-05,\n","              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1000, tol=1e-05,\n","              verbose=True)</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# Previsoes\n","\n","previsoes_census = rede_neural_census.predict(x_census_teste)\n","previsoes_census"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LSoZKMYBArh6","executionInfo":{"status":"ok","timestamp":1705957871017,"user_tz":180,"elapsed":262,"user":{"displayName":"Kaique F. Santos","userId":"18416165402555736361"}},"outputId":"e165afaf-4427-4451-d9f9-0354be71becb"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([' <=50K', ' <=50K', ' <=50K', ..., ' <=50K', ' <=50K', ' >50K'],\n","      dtype='<U6')"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["# Accuracy\n","\n","print(f'A taxa de acerto é de {accuracy_score(y_census_teste, previsoes_census)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dljdHb-oA1Ws","executionInfo":{"status":"ok","timestamp":1705957915995,"user_tz":180,"elapsed":263,"user":{"displayName":"Kaique F. Santos","userId":"18416165402555736361"}},"outputId":"47f09b0a-1c80-4d57-82c8-ea3d21a2fbdf"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["A taxa de acerto é de 0.8139201637666326\n"]}]},{"cell_type":"code","source":["# Matriz de confusão\n","\n","cm = ConfusionMatrix(rede_neural_census)\n","cm.fit(x_census_treinamento, y_census_treinamento)\n","cm.score(x_census_teste, y_census_teste)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":540},"id":"lcBhd-KqBBqO","executionInfo":{"status":"ok","timestamp":1705958045291,"user_tz":180,"elapsed":4,"user":{"displayName":"Kaique F. Santos","userId":"18416165402555736361"}},"outputId":"dccba4f2-4dee-487b-a907-1594b78770da"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8139201637666326"]},"metadata":{},"execution_count":27},{"output_type":"display_data","data":{"text/plain":["<Figure size 800x550 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAr8AAAH6CAYAAAAOZCSsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAqnUlEQVR4nO3de/zX8/3/8funFKWDIlJTDpHDcj6MrVIzhxlmYtook8Mc8kU5szHnnCdszpTSzA6k+VqykWMsaoYhYTQqoQM6fX5/9NvH97NPEerztj2v18vlc7n0eb5e7/fn8XLRp9vn9Xm9X++q6urq6gAAQAEaVHoAAACoL+IXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIqxQqUH+LIbP358qqur06hRo0qPAgDAYsybNy9VVVXZYostPnVf8fspqqurM2/evLz55puVHgVgmejYsWOlRwBYpj7Le7aJ30/RqFGjvPnmm3lqjwGVHgVgmfhO9QuL/vDOLZUdBGAZmfjGlku9r2t+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohviFZamqKtsf/6McMfHunDrnmZww9bH0GnF5WnZoV7NLh29slT7335ITpz+egW89kh/cc23W2GzDmu0du2+bn1a/sNiP7QccXLNfgxVWyA4D++WIv47MKbPGp/+L9+Vbg05M42Yr1+shA2XbeZ+LUrXqQZn82tTFbj/3krtStepBuXnYQ3W2vfLq1Gy701mpWvWgPP/3N5f3qJAkWaHSA8B/k50vPilbHrpf7jnizLz+8F/SulOH7P6Ls9L3gVszeMPdsuaWG6fP/Tdn4vB78of+Z2eFJitl50tOSp/7b87Vm3wns9+aVvNc123TK++9PqXW83/0/qyPv9YlJ2XLQ/bNPUeeldceejJrbvXV7HHd2WnWbvX89oCB9XbMQLluvO3BPDD2+SVuf+6FN3PBFaMWu+3Xd43LIf9zY9q1bbW8xoPF+lKd+T3wwAPTuXPnOh9bbLFFrf1efPHFHHLIIdliiy2yxRZb5NBDD83LL79ca5/OnTvn4osvrvM1hg0bls6dO2f48OHL9VgoT1XDhtlon53zyKDrM/G2u/Lu5H9k0uhH8qefXplW666VNTbtnK8dd1Dee21Kfv+jUzL1by9lylN/zd2HnJ6mq7bKV7//7VrPN3vqO5n91rRaH/M/+DBJ0rj5ytn6x/vn4QuvyzO3/DYzJr2ev93xh4wbPDRdeu+eFVs2r8R/AqAgU/75bgaccXsO77vjYrcvXLgwhxx7Y/ru//XFbj/+9OH5+fkH5MT+uy3HKaGuL92Z39122y2nnXZarbUGDT5u9BkzZqRPnz7ZZJNNcvvtt2fevHkZPHhw+vbtm1GjRqVFixZLfO5Ro0bl7LPPzoABA9K7d+/ldgyUqXrBglyxds+66wsXJkkWzpuXuw4+NY1WbpJUV9dsf/+Nt5IkjZs1XeqvNXfm7Fzavlvmzfmw1vr7b7ydqgYN0njlJvnovZmf5zAAlspRJw7JDtt2Sq89t85VN9xfZ/uV143O5NemZdSI4xe7fczvTkqndddY7OUQsDwtl/hdsGBBHnjggcyePTt77bXXZ3rsSiutlDZt2ixx+2233ZYPPvggl1xySVq2bJkkufDCC9OtW7cMHz48hx9++GIf9/DDD+fEE0/MwQcfnMMOO+wzzQSfV9vNN0q3M47MC3eNyVsTXkiSzJvzQa19Ou+5KJj/8djTn+m550ybUWet8549897rUzLzzbc/38AAS+GO3z+RP/7p2fzt0XPz8it1v99Mfm1qTjv3zgy5+rC0bLH4H+w7rbvG8h4TFmuZXvYwY8aMXHfddfnWt76V0047LQ0bNlyWT58kGTt2bLbYYoua8E2Sli1bZrPNNsuDDz642MdMmDAhRx99dPbee++ccMIJy3wm+Hc7XTAwp380MYc+eWcm/fHh/Gqf/ovdr2XH9vn24J/kpf99KK+MeazWtu2OOTCHPfWbnDD1sfz4mbuy5SH7JlVVS/ya2xz1w3TatWtGn1T3ch+AZeWdGbPS/+ShOf+MXlmr/aqL3eew427Orj27ZO/vbFXP08GnWyZnfv/2t79l6NChGTlyZNZZZ50cccQR2XPPPbPiiismSXbfffe8+eaSX8V53XXXZeutt16qr/XKK69kl112qbPesWPHjB49us76yy+/nMMOOyzdu3fPWWedtZRHBF/MwxfdkKdv+W3W3GLjfPP847Nq53Uy7NuH1VwCkSSrbbReDrzvxsx88+3c2XtAzfqCufMyc8rbadCwYe454sxUL6zOxr12yR7XnZNmbdvkwXOurvP1tvufvtnl0pPz4DnX5K/DR9bLMQJlOvbUYVm34+o5sl/dy7ySRS+CGzf+lTz36Hn1PBksnS8Uv+PHj8+gQYPyzDPPZKeddsr111+fbbfdts5+1157bebPn7/E51ljjY9/9fHaa6+lf//+mThxYubPn59tt902xx13XNZaa60kyezZs7PyynVv5dSsWbPMnFn7GscpU6akX79+mTFjRvbdd99a1w7D8vTB9Bn5YPqMTHvu5Ux74ZUc9uSd2WifXfK3O/6QJFnr61ul911X5+1nX8rtex6RD999v+ax/3h0fC5t17XW87355MQ0b79GvnHKYXl40HVZMHdezbadLjwhOww8OKNPviSPXHR9/RwgUKR775+QO0c+mSdHn7nYf1P/+da7GfiTEbnivB+m7Rqr1P+AsBS+UPyOHTs2L730Um666aZst912S9yvffv2S/V8LVu2zJtvvpnddtst/fv3z6uvvprLLrss+++/f+6+++60bt36M803cuTI7L333pk6dWoGDBiQO++8c6lngc+qyaqtsu43v5bJfx5X65Zlb//170mSNhuvlyRZc6uv5oB7r8vL9z2cO3sfXytkP8k/n34umx6wZ5q0XiWz/rnofpo7XTAwXzu2T3574ImZOOzuZXxEALWN+O0T+eCDeenS9fSater//wLeTluflAULFv126+BjbsjBx9xQ67H9/ufGHHLsTZn/9o31NzAsxheK327duuXxxx/PQQcdlO7du6dPnz7ZYYcdPvfzDR48uNbnG2ywQTbYYIPsvPPOGTZsWI4++ug0b948s2fPrvPYmTNn1roOOEn22muvXHDBBZkxY0b22WefHHnkkRk+fHiaNl36V9XD0mrUZMX0GnF5/njiRbXOwLb9/29gMfONt9K0Tev84J5f5uX7Hs4d+/5Prcsg/mWLg3tljU07595jz6213m6bLvlgxnuZ/fb0JMlmfb6b7QcenF9//7g8d+f/LscjA1jknFP3yYCjdq21Nu4vr+TgY27IqBHHp82qzdOoUd3X+3T5xun52cl7Z69vb1lfo8ISfaH43WyzzTJ06NC88MILue2223LUUUelffv2OeCAA7LXXnulSZMmSb7YNb8dO3ZM06ZN8/bbi15Nuu666+bVV1+ts9/kyZOz3nrr1VpbffXVkyStWrXKlVdemd69e+fkk0/OFVdckapPeOEQfB7v/+OfGX/Tnel2+hGZM/WdvPrguLTs2D67XnFqZk55O8/ecW92umBgVlixcUafdHGatqn9m4wFc+flwxnvZfbUd7Jt/wPSsHGjjLt6WBbMm59N9tstm+y7a8acfnmqFy5Mo6ZNsvMlJ+fZEaPy2tinsvIaq9V6ro/em5n5H35Un4cPFKB9u1Zp3672m1JMm77oksMN1lsja3dY8t2a2q/ZKl/d6CtJkrlz5+edGYvetOe9mYvugDPtnVn551vvpmHDBmmz2pJvWwpf1DJ5wVvnzp3zs5/9LAMHDsxvfvOb3Hjjjbn00ktz+umnZ88991yqa36nTZuWSy65JN/73veyzTbb1Gx7+eWXM2fOnKy99tpJku7du2fw4MGZMWNGWrVa9Bdw2rRpefrppzNw4JLf1WqTTTbJmWeemVNOOSVXX311jjrqqGVx6FDLPT/+aWa+8Xa6nXFkWnxljcz657S8+tBTGXPaZfnovZlZb5dvZKVVWqT/i/fVeezkPz2eW3r0yd/vHpMRex+dHU48JAc9ODSNmqyUaS+8knuOODNPXTsiSdJu66+m6Wqt0uUHe6TLD/ao81y/O+jkPHPLb5f78QJ8Ho888WJ67HVhrbWuuy96gVzHtVbN5KcvqcRYFKKquvr/3G1/Gamurs6DDz6YWbNmZffdd1/qx/Tq1SvTp0/P6aefns6dO+f111/PBRdckHfeeSd33313WrVqlZkzZ2b33XfP+uuvnxNPPDFJcv755+e1117LyJEjay5p6Ny5cw499NA6QXzmmWfm9ttvz+DBg7PTTjt96lwTJ07Mq6++mqf2GPCp+wL8J/hp9aJ7TuedWyo7CMAyMvGNRZfUdOnS5VP3XS63P6iqqkr37t2XOnz/9ZjrrrsuPXv2zHnnnZfddtstxx9/fDp16pThw4fXnOVt3rx5hgwZkhVWWCH7779/evfunZVXXjm33nrrUl3Le+qpp2azzTbLCSeckL///e+f+xgBAPjPs1zO/P43ceYX+G/jzC/w36biZ34BAODLSPwCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUY4VKD/Cf4opWUys9AsAy8dN//aF130qOAbDsvDFxqXd15hegMK1bt670CAAV48zvUujYsWPeeemySo8BsEy07nRcWrdunemPHVrpUQCWiVdf7ZqOHTsu1b7O/AIAUAzxCwBAMcQvAADFEL8AABRD/AIAUAzxCwBAMcQvAADFEL8AABRD/AIAUAzxCwBAMcQvAADFEL8AABRD/AIAUAzxCwBAMcQvAADFEL8AABRD/AIAUAzxCwBAMcQvAADFEL8AABRD/AIAUAzxCwBAMcQvAADFEL8AABRD/AIAUAzxCwBAMcQvAADFEL8AABRD/AIAUAzxCwBAMcQvAADFEL8AABRD/AIAUAzxCwBAMcQvAADFEL8AABRD/AIAUAzxCwBAMcQvAADFEL8AABRD/AIAUAzxCwBAMcQvAADFEL8AABRD/AIAUAzxCwBAMcQvAADFEL8AABRD/AIAUAzxCwBAMcQvAADFEL8AABRD/AIAUAzxCwBAMcQvAADFEL8AABRD/AIAUAzxCwBAMcQvAADFEL8AABRD/AIAUAzxCwBAMcQvAADFEL8AABRD/AIAUAzxCwBAMcQvAADFEL8AABRD/AIAUAzxCwBAMcQvAADFEL8AABRD/AIAUAzxCwBAMcQvAADFEL8AABRD/AIAUAzxCwBAMcQvAADFEL8AABRD/AIAUAzxCwBAMcQvAADFEL8AABRD/AIAUAzxCwBAMcQvAADFEL8AABRD/AIAUAzxCwBAMcQvAADFEL8AABRD/AIAUAzxCwBAMcQvAADFEL8AABRD/AIAUAzxC/Vo530uStWqB2Xya1Nr1ubM+SjHnTYsbTc6JiuvdVi23+XsPPDQc7Ue99fn/pG9fnhF2m50TFZc85Bs3v2M/Op3T9T3+EDhJv/jvTToPGiJHzf/ZmKS5KEnX883+9yeVbf9edbYfnB2P/TXefq5t5b4vC+9OiMrb3Zpehw4vL4OhYKtUOkBoBQ33vZgHhj7fJ313of+Ii+8NCV33HhU1mjTMudccld23e+SvPD4+Vm7Q5u8OWVGuu9xfrbbar38YcSArNx0xQz/zWP5fr+r07BBVfbZc5sKHA1QorXWbJ43xx5ZZ/3+R1/NIafdm65bfyWPPf1mdjpoRHrvvlF+fsZO+eDDeRl44QPZ6aAR+evIg9O2TbNaj62urs6hp9+befMX1tdhULgv1ZnfK6+8Mp07d17sx8SJE2v2e//993Paaadl++23T5cuXbL33nvngQceqPVcBx54YPbbb786X2PChAnZYostMmDAgCxc6C8a9WPKP9/NgDNuz+F9d6y1/tCjL+Sue8fn9uuPSNftO2eDTm3zy0sPynWX/SgrNm6UJLnr3vF5Z8bs/OKSvtli047ZoFPb/PTE72bD9dfMrSMeqcDRAKVq2LBB2rZpVutj1VWa5NxrHs3/9N0663VolctvHpcOa7bIjed/O5usv1q27rJmrjtn17zz7ocZMaruCYBf3v50Xnjlnez1zU4VOCJK9KU789u2bdv8+te/rrPeqlWrmj/3798/b7zxRi6//PKsttpqueuuu3LUUUdlyJAh2WqrrZb43C+//HIOO+ywbLfddrnwwgvToMGXqv35L3bUiUOyw7ad0mvPrXPVDffXrN9595PZaIN22bxLx5q1Jk0ap8/+X6/zHA3/7f/XFVf80v31BQp0+S1PZsb7H+W0H38tSXLDebtl9gfz0qBBVc0+7ddoniSZNWdercf+458zc9JFf8715+6aUX+elGkzPqi/wSlWvdTfvffem1GjRmX+/Pmfum/Dhg3Tpk2bOh8rrLDoH/px48blsccey5lnnpntttsu6623Xo477rh06dIlV1999RKfd8qUKenXr1/WX3/9XHHFFTXPB8vbHb9/In/807P5xSV962x7+q+vZcP118wtw8dms25nZNVOR6Xbd87L+Amv1uzTa89t0ma15jnxrBGZNevDVFdXZ9ivH81fn3sjhx+0Yz0eCUBts+fMzUXXP5EBB2+T5s1WTJKs3LRxVl915Vr73TXmpSTJ1zZvV2v9iJ/+b3ps1yH77rZh/QwMqaf4bdy4cc4777z06NEjV111VaZNm/a5n2vs2LFZaaWV8rWvfa3WeteuXfPYY49l7ty5dR4zY8aM9OvXL61bt84111yTFVdc8XN/ffgs3pkxK/1PHprzz+iVtdqvWmf7W2+/nyf+Mim3/frRXHNxn/z21mOSJF//9rl5/Y3pSZLVVm2eB35/ch4d93JarH1EVlzzkBx8zA254YqD8+1vbVavxwPwf133qwlZsLA6h39/yd+LJv/jvfT/2ejs/I21883tP/4t1213PZuHnvxHrjrzW/UxKtSol/jt2bNnxowZk4EDB+ZPf/pTdtxxxwwcODDPPPPMZ36uV155JWuuuWadM7cdO3bM/Pnz89prr9VanzNnTg4//PBUV1fn+uuvT7NmtS+0h+Xp2FOHZd2Oq+fIfj0Xu33e/AV59/05+fVNR2eHbddPtx0659c3HZ3q6upcPPjeJMlbb7+Xvfv8PJ3WWT33//bEPDzqtAw4ctf8eOAtufve8fV5OAC1/PzWp/KjfbrUnPX9d397aVq6/uC2tFu9WYZdskfN+tR35uTYc8fkgoHday6JgPpSbxe9Nm7cOHvttVfuuOOODB06NNXV1fnhD3+YffbZJ48++mjNfh9++GF+9rOfZdddd812222XAw88MI8//njN9lmzZmXllVeu8/z/itqZM2fWrM2fPz/9+/fPM888k5133jmtW7dejkcItd17/4TcOfLJ3HDFwUu8vrxliybZYL22adGiSc3a6m1aZMP118wzzy76Qe6iwX/I21Pfz29u6Z8eXTfKNluum3NP75Vde3bJCT8dUS/HAvDvnpw4JZPfeG+JL1Qb++Q/0vUHw7LOV1bJn4b2TutVPv4+d8zZo7PZhm1y+P6b19O08LGKXPi6+eabZ/PNN0+fPn1yxBFHZMyYMdl+++3TtGnTrLTSSunQoUO+//3v5913381NN92Ugw46KLfccku23Xbbz/R1nn322Xz1q19N3759c+2112bzzTdPjx49ltNRQW0jfvtEPvhgXrp0Pb1mrbq6OknSaeuT0n2HztlgvbZ5dNxLdR67cGF1WjRf9A/Fc39/M2t3WC3Nmq1Ua5/Ondpm5H3PpLq6OlVVVXWeA2B5+u3oF9Oq5UrZYYv2dbY9OXFKdjv0juz89bUz7NI9smLj2rkxYtTzadCgKo03ubhmbeHC6lRXJ402vig3nLdb+nz3q8v9GChTReL3ySefzK233prRo0enS5cu2WmnnZIk/fr1S79+/Wrtu+WWW2bXXXfN4MGDc+utt6Z58+Z544036jznv874tmjRomZt7bXXzrBhw9K4ceO8/vrrGThwYH71q19lvfXWW45HB4ucc+o+GXDUrrXWxv3llRx8zA0ZNeL4rL/uGhn72N9z+28ez1+emZwtN1s7STJ12vt54aUp2WOXzZMkHb+yWh5+/MXMmfNRmjb9+FeLz/19Sjp8pbXwBSrigcdey3abrpmGDWv/Zuvt6bPzncPvzM5fXzu/umKvOtuTZMLdP6qzdsblD+WNt2blxvN3y1fauhSC5afe4nfu3LkZOXJkbr311kyaNCm777577rjjjmyyySaf+LhGjRqlU6dOmTx5cpJk3XXXzQMPPJB58+alUaNGNftNnjw5jRo1SocOHWrWWrZsWfPitkGDBqVXr1454ogjcscdd6Rly5bL/iDh/2jfrlXat2tVa23a9EU/pG2w3hpZu0ObfKVdq1w0+A/Z/9BrMuTqw9K48Qo58cxfZcXGjXLMYYteBHLEwT1y/dA/54AfX5vTB+yRZiuvlLvuHZ+R9z2dc07dp96PCyBJnp80PT/YY+M66z+5Ymw+mrsgFwzsnqnvzKm1rXGjhmm9SpN8dYM2dR63SouV8u7Mjxa7DZaleonf0aNH54wzzkiTJk2y//77Z9999611395/ufDCC9OhQ4f07t27Zm3u3Ll5/vnns9FGGyVJdtxxx1x99dV55JFH0r1795r97r///nTt2rVWEP9fzZs3z+DBg7Pffvvl+OOPz7XXXpuGDRsu4yOFz6ZRoxXyxztPyMCf3J5d97skH82dn+23Xi9P/PEnWb3Not9idNl4rdz7qwH52cW/T7c9zs+8eQuy3tqr57JzfpD+h+1U4SMASrRwYXXeff+jtGxe94Vu942dnPdmfpTOu1xfZ1v3bdfKA0N611mH+lRvZ37PPvvs9OjR4xODs7q6Oueee24WLFiQrl27ZtasWfnlL3+ZqVOn5uKLF10XtNlmm6VHjx4566yzcv7556ddu3YZOnRoXn755Zx33nmfOMP666+fc889N8cdd1wGDRqUU045ZZkeI3yaHb+xUaqn31xrbY3VW2bILw7/xMf17LZxenare4YFoBIaNKjKwhdOXOy2SWM++fvZktx0wbe/yEiw1Oolfv91Te+nOeGEE7Laaqtl+PDhufjii1NVVZUuXbrkxhtvzDbbbFOz3yWXXJJBgwbl2GOPzaxZs7LRRhvlhhtu+NRLKJLk29/+diZMmJCbbropG264Yfbee+/PfVwAAPxnqar+18vPWayJEycmSbq0/0uFJwFYNlp3Oi5JMv2xQys8CcCycc/fu6Zjx47p0qXLp+5bb/f5BQCAShO/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxRC/AAAUQ/wCAFAM8QsAQDHELwAAxaiqrq6urvQQX2Z/+ctfUl1dncaNG1d6FIBl4tVXX630CADLVJs2bdKoUaNsueWWn7rvCvUwz3+0qqqqSo8AsEx17Nix0iMALFPz5s1b6mZz5hcAgGK45hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIX/iSeumllyo9AsAy9bvf/a7SI4D4hfo0YsSIpdpv9OjR+f73v7+cpwH44gYOHJiFCxd+4j7V1dW54IILcsopp9TTVLBk4hfq0U9/+tNcf/31n7jPNddck/79+2f99devp6kAPr8xY8bk6KOPzty5cxe7febMmTn00ENz880356CDDqrf4WAxxC/Uo5/85Ce59NJLc+mll9bZ9uGHH+bYY4/NFVdckf333z9DhgypwIQAn83NN9+c8ePH59BDD83s2bNrbZs0aVJ69eqVp556KpdeemlOOumkCk0JH6uqrq6urvQQUJJRo0blpJNOyve+972cddZZSZIpU6bkyCOPzKRJk3LmmWdm7733rvCUAEvv5ZdfziGHHJLVVlst119/fVq2bJk///nPGTBgQFZdddVcddVV6dSpU6XHhCTiFyri4YcfTv/+/dOjR4/06tUrxx9/fJo2bZorr7wyG2+8caXHA/jMpkyZkn79+qWqqirf+ta3cu2112bHHXfMoEGD0qxZs0qPBzXEL1TIhAkTcvjhh+fdd9/N17/+9Vx88cVZZZVVKj0WwOc2Y8aMHH744Zk4cWL69u2bk08+udIjQR2u+YUK2XTTTXPbbbelbdu2adOmjfAF/uO1atUqt9xyS3bYYYc89dRTmTdvXqVHgjpWqPQAUJLFvdBtq622qrn3ZZs2bWrWq6qqctxxx9XXaACfy/77719nbd68eXn22Wez5557pmXLlrW23X777fU1GiyWyx6gHm244YZLvW9VVVWee+655TgNwBd34IEHfqb93cmGShO/AAAUw2UPAMAyMX/+/Lz66quZNWtWkqRFixbp0KFDGjZsWOHJ4GPiF+rZrFmzMmzYsDz00EOZNGlSZs6cmWTRPxKdOnVKz549s99++2WllVaq8KQAS2f8+PG56qqr8thjj2XBggW1tjVq1CjdunXL0Ucf/Zku/YLlxWUPUI8mTZqUvn37ZubMmdlss83SsWPHrLzyykkWRfHkyZPz9NNPp23btrnlllvSrl27Ck8M8Mn+9Kc/5aijjkqXLl3StWvXdOzYsea+vjNnzswrr7ySMWPGZNKkSbnxxhuz9dZbV3hiSid+oR4ddthhadCgQQYNGpQWLVosdp9p06Zl4MCBadGiRX7+85/X84QAn833vve9dO3a9VPvTnP++efnmWeecbcHKs59fqEejRs3Lsccc8wSwzdJVltttZxyyil55JFH6nEygM/npZdeyne/+91P3e+AAw5wBxu+FMQv1KOqqqo0btx4qfZbuHBhPUwE8MU0a9Ys06dP/9T9pkyZ4m2O+VIQv1CPttpqq1x00UU1r4RenPfeey+DBg3KtttuW4+TAXw+PXr0yKmnnppHH310sT+0L1iwIA8++GBOPfXU7LzzzhWYEGpzzS/Uo5deeil9+vTJBx98kC233DJrrbVWrRe8vfbaaxk/fnxWWWWVDBkyJGuttVaFJwb4ZDNnzszRRx+dxx9/PE2aNMmaa65Z6/valClT8tFHH6V79+657LLL0qRJkwpPTOnEL9Szd999N0OHDs3DDz+cV155pdb9MNddd9107949vXv39utB4D/KuHHjMnbs2LzyyiuZPXt2kqR58+ZZd911s+OOO2bTTTet8ISwiPgFAKAY3uQCvgTefffdDBs2LG+99VbWWWed7L333mnZsmWlxwL4VM8++2w22mijNGhQ+2VETz75ZAYPHlzzfa1fv37ZaqutKjQlfMyZX6hHW265ZUaPHp3WrVvXrL3++uvp3bt3pk2blqZNm2bOnDlZffXVM3z48LRv376C0wJ8uo022ihjx47NqquuWrP2xBNP5KCDDkq7du3SqVOnPP/885k2bVpuuummbLPNNhWcFtztAerVnDlz8u8/b15++eVp2bJl7rvvvvzlL3/JPffck1atWuWyyy6r0JQAS29x59CuvPLKdOvWLffee29+8Ytf5I9//GN69uyZq666qgITQm3iFyrs8ccfz3HHHZcOHTokSdZbb72cdNJJ3uQC+I/14osvpl+/fllhhUVXVzZq1CiHH354Jk6cWOHJQPxCxTVq1Chrr712rbUOHTp84r2AAb7MWrVqlVVWWaXWWvPmzb15D18K4hfqWVVVVa3Pu3TpkhdffLHW2vPPP582bdrU51gAn0tVVVWd72s77LBDnd9ePfTQQ/nKV75Sn6PBYrnbA9Szc845JyuuuGLN59OnT8/111+f3XbbLcmiV0ifd9556dmzZ6VGBFhq1dXV2WeffWrd7eHDDz/MSiutlL59+yZJbr/99lx44YU59thjKzQlfEz8Qj3aZpttMnXq1FprDRo0SLt27Wo+/81vfpPWrVvn6KOPru/xAD6zJX2vatq0ac2fX3vttfzwhz/Mj370o/oaC5bIrc7gS2b69Om1bhkEACw7rvmFCnrqqacyd+7cWp83b968ghMBfHGPP/54zj///IwbN67So0AdzvxCBW255Zb5/e9/n7XWWmuxnwP8J+rVq1emTJmSDh06ZPjw4ZUeB2px5hcq6N9/9vSzKPCfbsKECXnhhRdyzTXXZMKECXn++ecrPRLUIn4BgGVmyJAh2WWXXbLpppvmm9/8Zm699dZKjwS1iF8AYJmYPn167r333vTp0ydJ0qdPn4waNSrvvfdehSeDj4lfAGCZGDFiRDbeeONsuummSZKtt94666yzTu64444KTwYfE78AwBe2YMGCjBgxIgcccECt9QMPPDDDhw/3mga+NMQvAPCF3XfffVmwYEHNu1X+y3e+85188MEHGTNmTIUmg9rEL1RQ+/bts8IKKyzxc4D/FA0aNMjZZ59d53tY48aNc/bZZzvzy5eG+/wCAFAMZ36hAu66666MGjVqsdtGjhy5xG0AwBcjfqECmjZtmrPPPrvWWxsnyYcffpizzz47zZo1q9BkAPDfTfxCBfTs2TNNmjTJyJEja63//ve/zyqrrJJu3bpVaDIA+O8mfqECGjRokN69e2fIkCG11ocOHZof/OAHFZoKAP77iV+okH333TeTJk3Kk08+mSR59NFH88Ybb2Sfffap8GQA8N9L/EKFrLLKKtl9990zdOjQJMmtt96aPfbYw/W+ALAciV+ooAMOOCCjR4/OuHHj8uc//7nOOyMBAMuW+/xChfXu3TuTJk3KBhtsUOcaYABg2RK/UGFPP/10xo4dm27dumXTTTet9DgA8F9N/AIAUAzX/AIAUAzxCwBAMcQvAADFEL8AABRD/AIAUAzxCwBAMcQvAADFEL8AABRD/AIAUAzxCwBAMcQvAADFEL8AABTj/wFinKsokmrEwwAAAABJRU5ErkJggg==\n"},"metadata":{}}]},{"cell_type":"code","source":["# Tabela de classificação\n","\n","print(classification_report(y_census_teste, previsoes_census))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T0ELgqNpBgPh","executionInfo":{"status":"ok","timestamp":1705958082834,"user_tz":180,"elapsed":417,"user":{"displayName":"Kaique F. Santos","userId":"18416165402555736361"}},"outputId":"40e12708-b8d6-4533-c5be-501fe2429680"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","       <=50K       0.87      0.88      0.88      3693\n","        >50K       0.62      0.61      0.61      1192\n","\n","    accuracy                           0.81      4885\n","   macro avg       0.75      0.74      0.75      4885\n","weighted avg       0.81      0.81      0.81      4885\n","\n"]}]}]}